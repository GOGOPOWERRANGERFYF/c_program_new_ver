笔记：

### Lecture 0: 
编译器会用多种中间表示(intermediate representation),抽象语法树只是其中一种.
instruction 指示；(计算机的)指令.
semantic analysis 语义分析 或 (context-sensitive analysis 上下文相关分析)

一个优化编译器:
  front end 前端:
    letter 字母; character 字符;
    word 单词,字;
    sentence 句子;
    编译器scanning阶段：taking letters and forming word
    编译器parsing阶段：taking words and forming sentences
    编译器semantic analysis阶段：taking sentences and forming a tree
      |
      V
    IR (intermediate representation) 中间表示
      |
      V
  optimizer 优化器: 
    code optimization
      |
      V
  backend 后端:
    编译器code generation阶段：taking tree and generating code
    basic code generation
    instruction selection
    register allocation
    instruction scheduling

理论基础:
csc 135
finite automata
regular expression
grammar


### Lecture 1: Compiler Overview(1) Structure & Major components
P2 (does, not how to does):
variable n. 变量；adj. 可变的；
statement (文字)陈述
phase 阶段
certain adj. 确定的；
intermediate adj.中间的;(两地、两物、两种状态等)之间的;

three main phase or component of an optimizing compiler:
note: difference compiler have difference desings,并不意味着必须这样.

An intermediate format or an intermediate representation
(AST) abstract syntax tree is one of the intermediate representations that compilers use,
a single compiler may use mutiple intermediate representations to represent the code and
to do the code generation process.
the code generation process is going from a high level language to machine language.
and compiler don't do this in one step, so there are multiple steps and phases that are 
involved to get from high level to assembly code.

conceptual view(概念视图):
  the high-level representation (the source code)

  x = a + b -> [front end] -(AST)-> [optimizer] -> [backend]

  load a -> r1
  load b -> r2
  add r1, r2 -> r3
  store r3 -> x
  note: real assembly doesn't have variable names/doesn't have symbolic information
  once the compiler forms the abstract syntax tree, it no longer needs the symbolic information,
  in theory once the compiler constructs this abstract syntax tree, it no longer needs to keep the
  symbolic information, like the variable name. But in practice(实践), there are reasons for keeping
  symbolic information throughout the entire compilation process.
    * resons -> for debugging purposes
  for each variable, which in fact becomes a node in this tree, it will keep the symbolic information,
  the corresponding(adj. 相应的) variable names, so that it can implement the debugging functionality.
  But without that, we don't need it. in other words, if you just want to generate code, once you build
  a tree like this you don't need the symbolic information.
    (AST):
         = assignment operation
        / \
       x   + addition operation  
          / \
         a   b

  0xff...
  |   |
  +---+ certain base, the base is the base address of the stack, a pointer to the activation record.
  | c |
  +---+
  | b |
  +---+
  | a |
  +---+
  |   |
  0x00...

  a,b,x -> symbolic information
  frame or activation record
  the frame for that particular program (该程序的帧)
  the stack frame for that (特定函数)particular function 
  symbolic information -> implement and debug


  ***注意：编程语言并不是自然语言,所以很多自然语言的概念并不直接适用/对等,只是作为辅助理解。例如下面两个:
  part of speech(演讲): 词性，词类 -> 个人理解：词的性质，词的分类；parts of speech为复数形式...;
  sentence 句子;

  编程语言中使用:
  token的类型(我自己琢磨的,以后还要完善和修改的)?
  statement 语句;

  semicolon 分号；

scanner(扫描器) or lexer(词法分析器):
  [front end] 
  [  scanning  ]
  [  (lexing)  ]
  it's a low level analysis of the input program.
  you can view the whole program as one big string.(vt.把...视为)
  It will look at it as a string that consists of these characters and it will divide it up
  into tokens or lexemes. -> to be specific(具体来说) -> 
    x12 = a3 + b45;
  what the scanner does?
  it will identify the difference tokens, the difference words that this string consists of.
  x12 is the first token or lexeme.
  equal is the second token.
  question: how did it know that? How did it know that this point is the end of the first token
  and the start of the next token?
  (了解:编程语言要求空格让开发编译器的工作更容易,但让程序员工作更难.
        looking for special symbols or characters, that mean certain things, for example the equal sign,
        it's reserved for assignment operator, Yes, So this is reserved, but how did it know that this
        equal sign is not part of the variable name? Because only letters and numbers are valid variable,
        name, So when it sees equal or a space, that's the end of the variable name.
  )
  It knew that this is the end of this based on a certain rule.
  we will be studying these rules. And these rules, as we will see, will be defines in the form of
  regular expression. So there is a regular expression that defines what a variable name may consist
  of. that tool(scanner/lexer) does not allow an equal sign in the variable name. So letters and numbers
  are allowed in variable name. So it knew that these three characters constitute(vt.组成) one token which
  is a variable name, but this(等于号) does not belong to that variable name, so this must belong to a differenced
  token. And there is no other part of speech in the programming language that starts with an equal sign. So
  it identified this equal sign, the operator, as a token by itself. There is no part of speech in a programming
  language, there is no rule that says that has something which starts with an equal sign. So it knew that 
  this is the end of this token. and this is can not be, for example:
    equal a 3 can not be a token by itself.
    =a3 This is not a valid token because there is no rule.
    So in order for the compiler to recognize this, it has to have certain rules that define, what the parts of
    speech of this language are, what the words are.

    x12
    =
    a3
    +
    b45
    ;
  what the scanner or lexer does. How does it do that? We will be studying this later.
  We will define this using regular expressions and the implementation(实现,完成) will require using
  finite state automata. So this is what the scan.


parser:  
  [front end] 
  [  scanning              ]
  [  (lexing)              ]
  [  parsing               ]
  [  (syntactic analysis)  ]
  Parsing it basically builds words. It builds sentences out of words.(它造出句子 由 单词.)
  So the scanner produces a bunch(束,堆) of words and the scanner takes these words and tries
  to build sentences out of these words.
  In a regular human language we talk about the sentence. The equivalent of this in a programming language
  is statement. And the equivalent of the word is the token or lexeme.
  human language       programming language
  word                 token
  sentence             statement
  So the tokens can be viewed as words and statements can be viewed as sentences.
  The scanner or the lexical analyzer divided this(source code) up into these tokens.
    x12=a3+b45;
  (x12): So the first token is in a variable name or an identifier.
  (=): This is equal.
  (a3): This is another identifier or a variable name. 
  Identifier is the name of a variable or  a name of function,  plus another identifier,
  plus the semicolon, the end of statement.
  So the scanner said, okay we have an identifier equals an identifier plus another identifier.
  Is there a rule in the language that allows us to bulid a sentense, a valid sentence?
  Well, in the languages that you are familiar(熟悉) with there is a rule where this identifier(x12) on the left
  is the identifier that we are assign to. And whatever appears on the right is the expression. So we
  are computing an expression and we are assigning an expression to an identifier, the left hand side of this 
  assignment statement. So in this case, you know, if there is a rule that says that you can have this, this
  would be a valid statement in the programming language, just, you know, a valid sentense in a human language.
  Right? Now these rules, how do you think these rules are defined in the construction of compilers?
  >> As a grammar.
  So a grammar is a rule that we will be using to define, the syntax of the different legal(合法的) statements
  in a programming language. So there will be a rule that will tell us that you can assign an expression to an
  identifier. But if we have something like, for example, 
    x = a + b */;
  x equal a plus b times divide something like this, there will be no rule in the programming language, in the
  grammars for this programming language. There will be no rule that will not alllow you to have an expression
  that consists of a plus b times multiply(v.乘) and nothing. So this is an invalid statement syntactically.
  How do we define what is valid and what's invalid? By defining the right grammars. So this is one of the main
  concepts in this course. This is going to be one of the main concepts, how to write the right grammars, to define
  the syntax of a programming language. So we will have a rule that will say this is good syntax. This is valid.
    x12=a3+b45;   (This is valid)
    identifier = identifier + identifier;
                      [expression]
    x = a + b */; (This is syntax error) <附加:个人理解...暂时还没想好,之后补充>
    Because this doesn't match any of the rules, any of the rules in the grammar that define this languege.
    重点:
    Okay? Any questions on this?
    Okay. So basically scanners are build using regular expressions and finite automata and parsers are built
    using grammars.
    So we will be studying difference algorithms for doing parsing. Parsing is one of the central topics in this
    course and in compiler construction.
    >>Okay? Any questions on this?

    >>Okay. Now the third part of the front end of the compiler is the semantic analyzer.
  
  semantic analyzer:
    [front end] 
    [  1.scanning              ]
    [    (lexing)              ]
    [  2.parsing               ]
    [    (syntactic analysis)  ]
    [  3.semantic analysis     ]
  Now, just like in human language, a sentence may be grammatically correct, but meaningless. You know, like
  saying the room drives the car. So when you say room drives the car, grammatically this is correct. This is
  correct syntax. But semantically this is meaningless. And in programming language, it's the same. So, you know,
  if we remove this bad stuff here, 
    x = a + b;
  this is valid statement, This is a simple expression assigned to a variable. So this is a valid statement, but
  int fact, it will only be valid if there types, if the types of these variables are consistent(符合的).
    x = a + b;
  So if we have integer a, b, x, all of them are integers.
    int a, b, x;
  then this is perfectly meaningful. Right? But if instead of having them integers, you know, 
  we have, you know, x is an object, a is an object of class student and b is an object of class car, and x is an
  integer.
    Student a;
    Car b;
    int x;
  So in this case, the types are not matching. They are not consistent and this is meaningless unless you have
  int you language, you are defining what it means to add a student and a car. And then what does it mean to 
  assign this to an integer? So here the types do not match, which means that this statement is syntactically 
  correct, but semantically it's not meaningful, doesn't have any meaning, which means that the compiler will
  not know how to generate code for this. So the compiler can generate code only for meaningful statements. If
  the statement is meaningless, it will not be able to generate code for it. So this is what compilers do in the
  semantic analysis phase, which follows the parsing. 
  So if you program passes scanning, it means that all the tokens, all the parts of speech are correct.
  If it passes parsing, it means that it's syntactically correct, but we haven't yet checked if the semantics are
  good or not. Then only after it passes semantic analysis, it will be a valid program. 
  An important part of semantic analysis is making sure that the types match. 
  And as you can see, parsing or checking syntax, can be done locally(adj.局部的). So we just look at this and match
  it up with one of the rules in grammar that defines the language. But in order to check semantic, we have to do a
  deeper and a more global kind of analysis. So we have to link, this assignment statement with these declarations.
    Student a;
    Car b;
    int x;
  which may have appeared much earlier in the code. So we have to link something that got declared at some point. And
  between this declaration and this statement, we may have a whole bunch of code.  
    Student a;
    Car b;
    int x;
    ... a whole bunch of code...
    x = a + b;
  Right? So now the compiler needs to link to find for each variable that is used here the corresponding type.
  (英文温习...动词+ed,过去时或被动式) And in order for compilers to do this, when they process a declaration, they're
  going to create an entry(n.条目) for each variable in what we call the symol table. So there is a symbol table. So these 
  declarations go into the symbol table. 
    Student a; \
    Car b;      + --> symbol table
    int x;     /
  So that symbol table is going to have information about each declared variable, its(pron.它的 it的所有格) type, its scope, 
  all the difference information that the compiler needs. And then when this variable is used, the compiler is going to
  look up that variable in the symbol table and see what type of variable that is. And based on those types, it will be 
  able to dicide whether this statement is semantically correct or not, whether this statement is meaningful or not.
  we will spend more than half the time in this course, doing this, studying algorithms for doing this, the frond end.
    x = a + b (this(1))
        |
        V
    [front end] 
    [  1.scanning              ]
    [    (lexing)              ]
    [  2.parsing               ] --+--> [optimizer] --> [backend] --> 
    [    (syntactic analysis)  ]   |
    [  3.semantic analysis     ]   |
                                   V (this(2))
                                assign
                                /    \
                               x      +
                                    /   \
                                    a    b
  So the frond end takes this(1) and gives us this(2). And of course it will gives us an abstract syntax tree only if
  the program passes all there phases.
  If the scanning is correct, if it passes scanning, if it passes parsing, if it passes semantic analysis.
  If it fails one of them, you will get an error message. The compiler is going to generate an error message.


Question: So now what does the compiler do in the optimization phase?
    x = a + b (this(1))
        |
        V
    [front end] 
    [  1.scanning              ]
    [    (lexing)              ]          <here(1)>      <here(2)>
    [  2.parsing               ] --+--> [optimizer] --> [backend] --> 
    [    (syntactic analysis)  ]   |
    [  3.semantic analysis     ]   |
                                   V (this(2))
                                assign
                                /    \
                               x      +
                                    /   \
                                    a    b
Now here(1) we are in the intermediate level. We have the code represented as abstract syntax tree or some other intermediate 
representation. By the way, the abstract tree is not the only intermediate representation.
Sometimes some compiler will convert the abstract syntax tree into some linear intermediate representation.
Compiler vary(v.变更) in the intermediate(adj.中间的,之间的) representations that they use.

Okay, Now in the optimization phase, the compiler be applying some transformations(n.转变) to improve the quality of the code, 
but in theory the optimization that are applied here(1) or the transformations that are applied here(1), they should be machine 
independent. So they are independent of the target machine that we are generating code for.  
So everything that is machine specific(adj.特定的,具体的,明确的,独特的), that is specific to the machine that we are trying to generate 
code for, should be ideally(adv.最适合地) done here(2).(这机器翻译翻得还不错,直接采用了...因此，所有特定于机器的，特定于我们试图为其生成代码的机器
的，都应该在这里完成.) And I say ideally because sometimes some machine specific stuff ends up(最终) being here(1).(机器翻译：我说这是理想的，
因为有时一些特定于机器的东西最终会出现在这里) But ideally this should be machine independent. Okay, maybe I should note here(2) that many 
compilers generate code for multiple machines, for multiple processors, multiple targets. So the extreme(n.极端;adj.极端的) example is GCC.
The GCC compiler, it generates code for many many difference machines and many many difference processors. So it's one compiler that 
generates code for many difference processors, which means that the backend is complicated(adj.复杂的,难懂的). Or you can think of it, 
for a good design there will be multiple backends or this backend will be organized in a way in which we have some common stuff that 
applies to all machines, to all processors and then some processor secific stuff. So you may look at, the backend may be divided into 
abstract or general stuff that applies to all processors.  
  [backend]
  [   abstract          ]
  [----+----+----+------]
  [ T1 | T2 | T3 |...   ]
And then this is specific to target number one(T1). This is specific to target number two(T2). This is specific to target number three. 
And so forth. So these targets are target processors, for example, Intel X86 is a target, ARM architecture is a target, GPU is a target,
imbedded processors, many many different(adj.不同的) kinds of processors. Okay. So here in the optimizer, I will just give an example of 
an optimization that is done here, one of the common optimizations. And this common optimization is called common subexpression 
elimination(n.消除). And I will just illustrate(vt.说明;(用示例,图画等)解释) this by example. 
  Common subexpression elimination (CSE)
  x = a + b + c;
  y = d/(a + b);
So if we have something like x equals a plus b plus c and y equals d divided by a plus b. So now given the name common subexpression 
elimination, what transformation do you think a compiler may apply on these two statements to make them, to make the code hopefully 
more efficient?  
>> A plus B is common between the two of them. 
>> Okay. So given the name, common subexpression elimination, 
a plus b is a common subexpression, right? 
So a compiler may do this. 
  x = a + b + c;
  y = d / (a + b);
      |
      | CSE
      | 
      V
    <transformation...>
  t = a + b; <temporary(0)>                   --+
  <between(1): 假设这里有 a bunch of code...>     | 
  x = t + c;  <here(1)>                          | <a long time(1)>
  <between(2)>                                   |
  y = d / t;  <here(2)>                       --+
So common subexpression elimination will just compute a plus b and put it in some temporary(adj.临时的;n.临时工). 
And again, don't worry about the syntax here. This is conceptual(adj.概念(上)的). This is the conceptual presentation(展示).
t equals a plus b. x equals t plus c and y equals what?
It equals d divided by t. So this is a transformation that may make the code faster.
This will not necessarily(adv.必然地) make the code faster. Generally speaking, there is no compiler optimization. There is no 
transformation that the compiler applies that is guaranteed(adj.必然的;v.保证) 100% to make things faster, because it may make 
things better from one aspect(n.方面,层面), but it may make something else worse. So in the case, for example, this will not 
necessarily improve the code because, sometimes it's, recomputing the subexpression maybe faster than, here(1), for example, 
loading this from memory. So this will improve performance assuming that (this?) t, this temporary(1) that holds a plus b, that 
t will in a register, not in memory. If this t is going to be in memory and we are going to load it here(1) and load it here(2), 
loading it from memory, loading from memory if it's not in the cache, in orders of magnitude slower that recomputing a plus b. 
a plus b is just an add operation that can be computed in a single cycle. But if we have to load this t from memory, well it's 
unlikely here because it's going to be in the cache, right, assuming that this will immediately follow this. So this T is going 
to be in the cache. But if we have a bunch of code in between(1), then this t may not be in the cache and we may have to bring 
it from main memory. Definitely(adv.肯定,明确地) if this(here(1)的t) is in main memory, this transformation will make things worse.
But even if this(here(1)的t) is in the cache, bringing this from the chache may take more time than recomputing a plus b. And even 
if it's in the storage(n.存储) location that is faster that the cache, which is what? What's the storage location that is faster 
than the cache? 
>> The register.
Even if it's in the register, so if it's in the register here, yes, this will work well, but this will require this t(在temporary(0))
to be reserving a register for a long time<a long time(1)>. And with this variable<t in temporary(0)> reserving a register for a long 
time, other variables may not have the chance to be stored in a register. So when this variable<t in temporary(0)> takes this register   
for this many instruction because we may have some instructions in between here<between(1)和(2)>, right? So keeping t in a register for 
this long time<a long time(1)>, this is reserving a precious(adj.宝贵的) resource, which is the register, which means that some other 
variables may not get to be in a register, so they will be in memory and there will be a slowness when using those other variables. So 
we victimized(v.使受害) other variables. So just, the point here is that any optimization, any transformation, I hate to say optimization 
because it's not guaranteed to make things even better, not to mention optimally. It's not even guaranteed to make things better. It 
could make things worse. But compilers do optimizations in the hope that they will make things better.  
You Know, in a good optimizing compiler, compiler optimizations make things better most of the time. Okay? So this is only one example 
of those transformations that are implemented in the optimizer, in the optimization phase. This is just one example, just to give you 
an idea. And the point here is that this kind of transformation, even(adv.甚至) though(conj.即使) it appears to be machine independent, 
So in principle it's machine independent, but if you think about it deeply, it will not be 100% machine independent. 
Because when you start talking about memory, caching, registers, whether this<transformation> will be good or bad will depend on how 
much cache we have, how many registers do we have, and parameters(n.参数;决定因素) like this.
That's why in a real compiler, in this middle optimizer, even though it's supposed to(v.应该) be machine independent, you will often 
find some machine dependent parameters. for example, 
common subexpression elimination is an optimization that requires variable to stay in registers for a long time and this will 
beneficial(adj.有利的) if you have enough registers. So on a machine that doesn't have very many registers, this may not be a 
good optimization. Okay? 


backend后端:
Now, the backend is the part that is truly machine dependent. 
It's the part in which we do instruction selection and instruction scheduling and register allocation.
    x = a + b
        |
        V
    [front end]                                         [backend]
    [  1.scanning              ]                        [instruction selection  ]
    [    (lexing)              ]                        [instruction scheduling ]
    [  2.parsing               ] --+--> [optimizer] --> [registers allocation   ]--> 
    [    (syntactic analysis)  ]   |                    [                       ]
    [  3.semantic analysis     ]   |
                                   V 
                                assign
                                /    \
                               x      +
                                    /   \
                                    a    b
So today I will just in the remaining(adj.剩下的) ten minutes I will give a brief overview of these, but in fact these are,
we'll spend two or three more lectures on these trying to understand, well at least three more lectures, trying to understand,
what these<backend?> compiler optimizations do. 

So instruction selection is selecting instructions. 
So to go from an abstract syntax tree assembly(n.), maybe let's write an expression that is more interestiong. 
So let's write a more interesting example like x equals a times b plus c times d. 
  x = a * b + c * d;
And in this case, the abstract syntax tree is going to be this, 
  AST
      assign
     /       \
    x         +
            /   \
           *     *
          / \   /  \
          a  b  c   d
a mutiplied with b. And c mutiplied with d, add these.  
So the abstract syntax tree is very logical, a logical, hierarchical representation of this expression computation and 
assignment.  
and then we are assigning this to x.

Now the assembly for this is going to be loading, 
  load
well, what kind of assembly? That's going to depend on the machine.
So it's common to first generate some abstract assembly, some generic assembly like, the assembly  
for some risk(risc?) machine because risk(risc?) machine do not have very many instructions. So you are generating 
basic assembly. 
That(load) is the instructions you are using likely(adj.可能的) to be almost on all processors. All processors will
have a load and an add. So using the basic operations, loading a into register r1, loading b into register r2, then 
multiplying what? r1 and r2 and putting the reult in r3 and then loading c into register r4 and loading d into register r5.
then multiplying r4 and r5. And putting it in r6. Then, adding r3 and r6. Putting it into r7. Storing r7 into x.
  load a -> r1  (r1-r7: virtual registers)
  load b -> r2
  mult r1, r2 -> r3
  load c -> r4
  load d -> r5
  mult r4, r5 -> r6
  add r3, r6 -> r7
  store r7 -> x
The first observation(n.观察) here is that we are assuming that we have as many registers we want.
So this is obviously a nonrealistic assumption.
So we are just taking the liberty(n.自由;许可证) to use as many registers as we want and compiler do this 
at some point before they get to(取得?) register allocation. So before they get to register allocation, before
it gets to reigster allocation, the compiler assumes that it has an infinite number of registers, which 
is obviously not true. There is no machine that has an infinite number of registers. The job of the register 
allocator is to take these registers here,these virtual registers. So these virtual registers. They are not 
real yet. They are not real registers yet. So the job of the register allocator is to take these virtual registers 
and map them into real registers or physical registers. 
Okay? And of course we'll say more about register allocation in the next lecture.
So register allocation is something that we need to understand. We need to understand the problem of register allocation. 
So this is one observation. So this is something that is done in the backend, mapping virtual registers into physical 
registers.  
Okay. So another observation is that, here we are generating this code sequence multiplying, 
loading a into a register,
loading b into a register,
then multiplying the registers.
But on some machines, there are complex instructions that may operate on memory operators. In fact, if you have 
a memory operation and a multiply operation that, multiplies a and b, which are memory locations, memory addresses,
  mult a, b 
and putting that in could be in memory and it could be in a register.  
So on ??? machine, you may have other options. 
You can replace this sequence of three instructions, load, load, and multiply, with one instruction that 
operates directly on memory operands. 
And maybe put this in r1. 
  mult a, b -> r1
In fact, the machine may have an arithmetic operation that takes two memory operands and puts the result in a memory 
operand. And that's a ??? instruction, a complex instruction, that in the machine itself consists of multiple 
risk(risc 字幕是不是错了?是否其实是risc:精简指令集计算机) simple instructions. 
  附加:
      ==> 精简指令集计算机(RISC:Reduced Instruction Set Computer);
      ==> 复杂指令集计算机(CISC: Complex Instruction Set Computer);
Okay. So what's the point here?
The point here is that which instruction will get selected and what's the most efficient sequence of 
instuction is the job of which phase do you think? 
Which phase does this? Yeah. So this instruction selection does just that. 
  [backend]
  [instruction selection  ] <---- 
  [instruction scheduling ]
  [registers allocation   ]
It selects the best sequence of instructions for this, intermediate representation(abstract assembly).
So you can think of the abstract syntax tree as a high level intermediate representation.
  (risc?)
  load a -> r1  (virtual registers) -+
  load b -> r2                       |-> mult a, b -> r1 (cisc?)
  mult r1, r2 -> r3                 -+
This abstract assembly as a lower level intermediate representation, which is not quite machine code yet(还不是完全的机器代码),
but when it goes through instruction selection, instruction selection will select real instructions, actual instructions.
So as you can see, we are making things more real. we start with virtual and abstract things and little 
by little, gradually(adv.逐渐地) we make them more real to end up with real code that will run on a real machine. Okay? So
this is instruction selection. 
Questions on the concept of instruction selection?

Okay. So we know what instruction selection is.

We know what register allocation is.

It remains to say something about instruction scheduling.
And instruction scheduling is given this sequence of instructions, this is not necessarily the best ordering for 
executing these instructions. Why? Because here, for example. So let's go back to this.
      load a -> r1
      load b -> r2
  +--> ... (n cycles)  
  |   mult r1, r2 -> r3 (this multiply(1))
  +---load c -> r4
      load d -> r5
      mult r4, r5 -> r6 (this multiply(2))
      add r3, r6 -> r7
      store r7 -> x
Load instructions in particular have latencies(延迟?潜伏期.).
So a load instruction may take a few cycles to execute. So when you get to this multiply(1), you will have to wait 
for the load, for the result of this load(mult r1...上面的两条load), to become available(adj.可获得的).
And this can waste two or three or four cycles, depending on the machine. 
But what if we instead of wasting these cycles, what if we put this load here(n cycles) to use this time  
to hide the latency(n.延迟) of this load and execute another load for this multiply(this multiply(2)).
So to hide latencies we can reorder instructions to hide latencies and get better performance. 

We will say more about this next time.
Okay? So next time we'll say more about instruction scheduling.


### Lecture 2: Compiler Overview(2) Register Allocation Concepts
OK. SO, we'll continue our overview of compiler. And today we will focus on register allocation and 
instruction scheduling. So, first, let's draw that picture that we do last time. 
So, the compiler, we have the front end. We have the optimizer. And, we have the backend.
      front end       optimizer        backend
    [           ]   [           ]   [           ]
--->[           ]-->[           ]-->[           ]
    [           ]   [           ]   [           ]
OK? And, let's try to remember, what each one of these does, just to review what we covered in the 
previous lectures. 
So, what does the front end do? The whole front end, What's the function of the front end? 
>> So, it's scanning, parsing, and semantic analysis.
      front end             optimizer        backend
    [scanning         ]   [           ]   [           ]
--->[parsing          ]-->[           ]-->[           ]
    [semantic analysis]   [           ]   [           ]
So, what do these all combined(adj.联合的;v.结合) do? What's the input and what's the output of the front 
end? 
>> Source code. The input is source code. 
And, what's the output of the front end? 
>> Abstract syntax tree?
>> Yes. An intermediate representation of the code. which is typically an abstract syntax tree. And, the 
abstract syntax tree is a tree representation of the code. And, we have seen an example last time. 

Now, in this middle part of the compiler we have the optimizer. 
So, last time, we gave an example of the optimizations that take place(进行) here(1).
What was that example? 
>> common subexpression elimination.
                              <here(1)>           <here(2)>
                         machine-independent   machine-dependent
      front end               optimizer            backend
    [scanning         ]      [           ]      [           ]
--->[parsing          ]----->[           ]----->[           ]
    [semantic analysis]      [           ]      [           ]
So, it's common subexpression elimination. And, last time we mentioned that optimizations that take place 
here(1) are intended(adj.打算的) to be machine-independent. So, they are not low-level optimizations that target 
a specific(具体的) machine, or that try to do good utilization(n.利用,效用) of the capabilities(n.能力) of the
hardware. Optimization that try to maximize(v.最大化;最大限度地利用;充分利用) and optimize the utilization of the 
hardware are here(2) in the backend. So, the backend is intended to have those low-level optimizations that 
make the best use of the available hardware, that optimize the code on a specific hardware.  
So, in principle, this(here(2)) should be machine-dependent. The backend is machine-dependent by nature. 
Why the middle optimizer is supposed to be machine-independent. But, last time we said that in practice, these 
optimization here(1) are not going to be strictly machine-independent. Their nature has nothing to do with the 
  --> to do with 和有关,有关; 对待,如何处理; 根据这里的上下文,这里的意思为有关. 它们自然与机器无关?
machine, it has to do with the code, right? 
And, common subexpression elimination, it's an optimization that has to do with the code itself. It's trying to 
eliminate some redundancy(n.冗余) in the code, or it's trying to do some re-use. That idea here in common 
subexpression elimination is reuse. Instead of computing something multiple times, compute it once(adv.一次), and 
then reuse it. That's the idea. 
  CSE
  reuse
Now, this reuse requires resources, So, when you reuse something, you want to store it somewhere. So, you have to 
store it somewhere, in some place, so that you can reuse it. 
But, that place that you store it has to be fast enough. It has to have easy access. If it doesn't have easy access, 
then that defeats(vt.使失败;n.击败) the purpose. 
So, it's if you don't have a fast enough device that you store this thing in, you will not do an optimization. It may 
be just faster to recompute, instead of resuing. And, having said that, this means that for this kind of reuse, 
the best storage device for this is the register. So, you want it to be in a CPU register, which is the fastest storage 
device. And, when you keep doing optimization like this, that require, reuse and storage, and require register in 
particular, you are increasing the demand(n.需要;) for registers. And, increasing the demand for registers is not a good 
thing, you know? Increasing the demand for registers may end up slowing performance. 
Why? Because you have a limited number of physical registers on the machine. So, this is something that we will face in 
the backend. There is a limited number of physical registers on the machine. And, if you have, like, 200 front(我听到的是different?),
variables in you function. Two hundred variables. And, you have 300 temporaries, why do you think that the generic(adj.一般的) 
code will have temporaries? 
  200 variables
  300 temporaries
So, there are the original variables that are in the source program, and there are temporaries that the compiler will 
generate. Why do you think those would ??? swap in and out to registers? 
So, let's look at our example. In fact, the example we will be doing today, a times b, plus c, divided by d, is assigned to x. 
  x = a * b + c / d;
??? Intermediate values. So, if we look at the abstract syntax tree, we'll have a and b, they will get multiplied, but the 
result of this multiplication has to be store somewhere, right? 
And then, c and d, they are divided. 
And then, you take the result and you add it, 
and then you assign to x. 
  this(1)
      assign
     /       \
    x         +
            /   \
  node(1)  *     /
          / \   /  \
          a  b  c   d
You assign this to x. 
Now, this is intermediate result 
You know when we wrote some the assembly code for this, we wrote the code like this.
  this(2)
  load a -> r1
  load b -> r2
  mult r1, r2 -> r3
load a into register r1. 
load b into register r2.
then, multiply r1 and r2. And, put the result in r3.
Now, in this code, we have three registers.
Register r1 holds(v.保存,容纳) an actual program varialbe.
Register number 2 has a program variable. 
Register number 3 has a temporary. It has a .. it's.. it doesn't correspond to a program variable. It stores some 
intermediate result, which is the result of the multiplication between r1 and r2. So, it corresponds to this node(1)
in the abstract syntax tree. 
So, in general, the code that is generated by the compiler, it's generated.. the first step in the code generation is 
this abstract syntax tree. And, some compiler may, immediately convert this(1) into some linear representation like this(2). 

重点:
Some compiler may delay this a little bit and work on the abstract, on the tree representation of the code. 
So, this(1) is what we call a tree representation, or a graphical representation of the code. And, this(2) is linear,
a linear form, a linear representation of the code. 
And, last time, we said that usually a compiler uses multiple representations, multiple intermediate representations, 
throughout the compilation process. So, it will start with typically starts with a tree representation that moves to 
a linear representation like this(2), which is abstract assembly. Then it will start at making this abstract assembly more 
real, targeting the real machine.  
So, here we have a program with some, 200 variables, and 400 temporaries. And, the machine has registers.
  200 variables
  400 temporaries
  ----------------
  16 registers
So, you can not put all the variables and all the temporaries in cpu registers. 
So, there is a competition for registers. In fact, this is going to be the main topic in today's lecture. 
But the point here is that in optimizations may require more registers. They increase the demand for registers. 
They increase what we call the register pressure(n.压力). And, when that register pressure increases, not all of 
these virtual registers can be stored. So some of these virtual registers may end up in memory.   
We will see this in detail in today's lecture. 
But, when they end up in memory, that's slow. So, this means that, we could end up slowing the execution. 
Some optimizations that appear(v.显得,呈现) to be good, and they are creating redundancies(n.冗余;), or they are 
doing a lot of reuse, they may end up increasing register pressure, increasing the competition for registers and 
some of the variables on the temporaries will not be stored in registers, they will be placed in memory. And, in 
that case, our execution may slow down.   
这段AI翻译供参考:
(然而，当它们最终存储到内存中时，执行速度会变慢。这意味着，某些看似有益的优化措施实际上可能会造成冗余，或者过度地重用资源，这反而可能
导致寄存器压力增大，加剧了对寄存器的竞争。这样一来，一些临时变量将无法存储在寄存器中，而会被放置在内存中。在这种情况下，程序的执行效率
反而会下降。)

OK. So, in the backend, we have instruction selection. And, instruction scheduling, and register allocation.
                           machine-independent   machine-dependent
        front end               optimizer            backend
      [scanning         ]      [           ]      [instruction selection ]
  --->[parsing          ]----->[           ]----->[instruction scheduling]
      [semantic analysis]      [           ]      [register allocation   ]
    
    this(1)
  load a -> r1  first(1)
  load b -> r2  second(1)
  mult r1, r2 -> r3

                        this(2)
                        assign
                       /       \
                      x         + here(1)
                              /   \
                             /     \                
         +-  left branch(1) * it(1) /  right branch(1)   -+
this(3)  |                 / \     /  \                   | this(4)
         +-               a  b     c   d                 -+
  leaves: a b c d
So, we talked about instruction selection last time.
So, what's the instruction selection?
>> OK. The best sequence of instructions for the given code. 
So, you can either(adv.而且) apply instruction selection.. you can apply it to a tree representation of the code. 
You can apply directly to a tree representation of the code, or you can apply it to a linear representation of 
the code like this(1).
And then, you want to decide, what are the best, or what are the best machine instructions for implementing this.
So, this(1) could by one-to-one mapping. We could just map these abstract assembly instructions to actual, 
concrete(adj.具体的,而非想象或猜测的) instructions on the large machine. This is possible. But, sometimes this may 
not be the most efficient way.  
Sometimes, what was the example we presented -><this(1)> last time? 
>> The load taking multiple cycles, so rather than having a multiple weight on the load, have another load between 
>> those two to pick up the..
OK. So, that was instruction scheduling. But, now we're talking about instruction selection. In instruction selection, 
what wat the a better sequence for these three instructions? On the target machine?
>> You said there was ??? machines that the multiply didn't couldn't do it for two loaded variables rather 
Two memory locations, yes.
So, if the machine, we are doing this assuming that the machine does not have a multiply instruction that can 
operate on memory. on memory operands. But if that machine, if the target machine has an instruction  an assist 
instruction(辅助指令?) that can operate on memory operands, we can replace this sequence of three instructions 
with one instruction that operates directly on memory operands. Instead of loading the first(1), loading the second(1),
and the multiplying. 
So, instruction selection is about utilizing(v.利用), making the best use of an instruction set that is available 
on the target processor. Making the best choices and using selecting the most efficient sequence of the machine 
instructions for this(1), intermediate representation. Remember that this(1) is still, this(1) abstract assembly 
is still an intermediate representation. 
By the way, can you think of a.. can you think of an algorithm that takes you from this(2) tree that converts 
this(2) tree into this(1) abstract assembly? 
>> Like, just reversing the tree if 
>> Reversing the tree. 
What kind of reversal will give us this?
>> I'm forgetting the words for this. 
OK. So, what kind of reversal will help us in this case? Will get us from this(2) abstract syntax tree to 
>> 
Yes, it's depth first. And, can you explain why depth first? 
>> Because you got to(开始?) go down the tree to hit a, And then, you got to back up and hit b for the ??? 
>> and then multiply them together for the operation. 
OK. So, we need the depth because if we are here(1), we want to perform the add. We can not perform the add 
until we completely compute this left branch(1) and this right branch(2). So, we can not do the add until we 
do everything below(prep.在下面) it. Until we completely compute this(3), and completely compute this(4). And..
but, we can not compute the multiply.. we can not compute the multiply until we do everything below it(1).
Everything beneath(prep.在下方) it, right? So, this is a depth first(深度优先) by nature. 
In this case, we have.. these are the leaves(n.叶).
So, leaves are loads, because we're loading these variables. 
Then, when we get to the multiply, you multiply register 1 with register 2, and then,
    abstract assembly(1)
  load a -> r1
  load b -> r2
  mult r1, r2 -> r3 result(1)
  load c -> r4
  load d -> r5
  div r4, r5 -> r6 result(2)
  add r3, r6 -> r7
  store r7 -> x 
you load c into register r4, And load d into register r5. 
And divide r4 and r5. And, put the result in r6. 
  abstract syntax tree(1)
           assign  store
          /       \
         x         + here(1):r7
                 /   \
r3:this(1)  +-  *     /      -+ r6:this(2)
            |   / \   /  \    |
            +-  a  b  c   d  -+
Then.. so we have traversed this(1), and now we are here(1). So, we need to do add.
So, this(1) result here is in register r3, and this(2) is in r6.
Right? So, dividing our 4 and our 5, we have this result(1) in r3 and this result(2) in r6.
And then, we need to add r3 and r6.
And, put the result in some other register, like r7. And now the result is here(1) in r7. 
So, what should we do with it?
Yes, store it in.. so, the assign is a store. 
We will cover this in more detail when we get to code generation, but this is just, in this case it's 
just so intuitive(adj.直觉的,直观的). this straightforward(adj.直截了当的) conversion from this abstract 
syntax tree(1) to this abstract assembly(1).
store r7 into x. OK.
So, here.. the most important observation here is that when we are constructing this abstract assembly, 
we are assuming that we have an infinite number of registers. 
So, this assumption is one of the main things that make it abstract assembly. So, it's not real assembly yet.
It's not real because we are assuming that we have as many registers as we want. 
So, what did we call these registers? 
Virtual registers. So, these are not real registers. We're just using as many registers as we need. 
So, these reigsters are virtual registers. 
What the register allocator does that the register allocation phase in the compiler.. what it does it, 
it maps these virtual reigsters into physical registers as we will see in a minute. 
OK? We'll see how it does this mapping of virtual registers into physical registers.
Now, I have to make a note about, these a's and b's that I'm using in my assembly. I think I pointed out 
that real assembly does not have variable names. Right? So, real assembly machine are not aware(adj.意识到) of 
variable names. A machine doesn't know variables. 
Doesn't even.. is not aware of the whole notion(n.概念) of a variable. A machine knows storage locations. So, 
it knows registers and memory locations. That's all what the machine understands.
  (个人备注:变量名(标识符)是内存地址的别名)
The machine doesn't understand variables.
But, what are we implying(v.暗示) here? If you remember from lecture 1, or lecture 2.
When we are using this variable name here, what are we implying?
>> The address?
Yes, the address of this variable in memory.
And, in fact, let's introduce the notation(n.标记法) in the book.
So, in the textbook, the textbook uses a language, an intermediate representation called ILOC, which stands 
for Intermediate Language for an Optimizing Compiler. 
ILOC, Intermediate Language for an Optimizing Compiler.
ctivation record pointer address of a into r1. 
  loadAI r arp, @a -> r1 (this(1))          load a -> r1 (this(2))
So, what I'm writing here, this(2) is a shorthand(n.速记) for this(1). What does this mean?
So, this is load address immediate. So, this(loadAI) is a load instruction that takes an address, and it(loadAI) takes an 
immediate operand. So the address is ??? register. This register has a pointer to the activation record for 
the current routine that we are compiling. And, this address of a, this is an immediate number that represent 
what? It's just a number that offset, exactly. The offset of variable a. 
So, let's since we wil be using this, let's clarify(vt.阐明,使更清晰易懂) it even further(adj.进一步的;adv.进一步). 
So, if you have a main function. 
    |              |
    |              |
    +--------------+
    |              |
    |--------------|
    |     c        |
    |--------------|        this(3)
    |     b        | f2                r arp
    |--------------|        this(2)    +------------+
    |     a        |                   | 0xff00     |  register(1)
    +--------------+ 0xff10 this(1)    +------------+ 
    |              |
    |              | f1         a -> 0xff00
    |              |            b -> 0xff04
    +--------------+            c -> 0xff08
    |              |
    |              | main
    |              |
    +--------------+
You main function.. a program with a main function.
The main function will have an activation record on the stack.
This activation record has the local variables, and it has the parameters, and it has the return value, and it 
has the return address for every function. This is what we mean by the activation record. 
When main calls function 1, the activation record for function 1 will get pushed on the stack. When function 1 
calls function 2, the activation record for function 2 will get pushed on the stack. 
So, each function, each active function, as this location on the stack, that we call the activation record for 
this function, and it has the local variables of this function, it has the parameters of this function. It has 
the return value. And, it has the return address, and other things. So, this is the activation record.
So, when function 2 completes, what will happen? 
When function 2 completes executing, and needs to return to f1?
>> Yes. The activation record of f2 gets ???(bottom of:按常理和我听出来的应该是这两个单词) the stack.
So now, each of these activation records will have a, an address associated with it. 
So, if we are in function 2, the address for.. the address for function 2 could be, for example, 0xff10(马上要被修改为0xff00,不用管...). 
So, this is the starting address of f2. 
So， if we are compiling, if the compiler is currently compiling f2, then this is the starting address of..
let's make it 00. This is the starting address of function f2. And, the function f2, has variables a and b and c. 
So, the address of variable a is this starting address, which we assume is stored in a register that we call 
activation record pointer register. 
So, it's a special register that holds the starting address of the activation record of the current function. 
So, in this case, it's going to(将) have 0xff00. 
So, to compute the actual address of a variable, for a, it's going to be this.. 0xff00. That's going to be for, variable 
a. For variable b, it's going to be 0xff04. Assuming that we have 32-bit variables. This(1) is going to be, offset 0. This(2) 
is offset 4. This(3) is offset 8. Assuming 32-bit variables. 
And, for c, 0xff08. 
              this(1)
                |
                V
  LoadAI r arp, @a -> r1 this(2)         load a -> r1 here(1)
OK, SO, this is the notation in the book. So, a load instruction takes the activation register which is in.. for a local 
variable. The base if the activation record pointer. The address in the.. the register(1) that holds the activation record 
pointer. And, this(1) is the offset for the variable.
And, what I have here(1) is just a shorthand for this(2). it's understood that we're assuming that variable are local variables,
and all addresses are relative to that activation.. to the starting address of that stack frame. 
And. a is not a variable, it's in fact on offset. 
OK? Questions on this?
Alright. So usually we will just do this<here(1)> shorthand. We will not write the full syntax of the assembly in the book, 
or the abstract assembly. OK? Now, how does the compiler.. how does the register allocator map these virtual registers 
into physical registers?
        virtual registers
      +-  load a -> r1        here(2)           
      |   load b -> r2                          -+ live range(2):red(3)  definition(1)
live  |   mult r1 here(3), r2 -> r3   this(1)   -+                       last use(1)
range |   load c -> r4
(1)   |   load d -> r5
:red  |   div r4, r5 -> r6
(1)   |   add r3, r6 -> r7
      |   store r7 -> x 
      +-  add r1 here(4), r7 -> r8    >> here(1):整条抽象汇编指令:red(2)
Now, let's assume that we have.. or, the target machine.. has  three physical registers. 
If the target machine has three physical registers..
  P1, P2, P3
so let's call them P1, P2, and P3.
So, we have three physical registers of the target machine, and how many virtual registers do we have here?
Seven. So, the job of the register allocator is to do the mapping between these. So, how will it ma them in 
this case?
Well, it's going to say, OK, 
    allocation(1)
  r1 -> P1
  r2 -> P2
  r3 -> p1
register r1, I'm going to map r1 to P1, the first physical register. 
And, I'm going to map r2 to P2.
Now, here, obviously we need two physical registers.
Now, this(1) multiply now is multiplying r1 and putting the result in r3.
The question is, for r3, do we need a new register, or we can just reuse P1 or P2?
>> Reuse.
So, why can we reuse? Yes, we can reuse. 
>> Because they're not using it.
Exactly. Because we are not used again.
So, because r1 and r2 are not used again, we don't have to use a third physical register. We can just reuse r1 or r2.
So, we can say, OK, r3 is going to get mapped to P1. 
Because after this(1) instruction, we do not need r1 and r2 again.
To explain this further, assuming that we have assumed an additional instruction like this, add r1 and r7, and put the 
result in r8. Now, if you.. if we had an instruction like this, 
would have this allocation(1) been legal? No. It won't be legal. Now, here, for r3, you can not reuse.. you can not reuse 
the register for 1, because I still need it. I still need it here(1). 
So, I still need to keep r1 in a physical register until I get to(到达) this<here(1)> instruction.
So, basically, you always need to keep a virtual register in the same physical register until you are done with it. until the 
last use of that register.
So, with this instruction in red<here(1)> added, I want to be able to reuse P1, but I can still reuse P2. Right?
P2.. r2 is not used. So, with instruction in red<here(1)>, I can reuse P1..(说错了,马上找补...) I can not reuse P1, but I can 
reuse P2.
  r1 -> P1
  r2 -> P2
  r3 -> p2
So, in register allocation.. and, by the way, we are not discarding(v.丢弃) an algorithm now. We are discarding the concept.
  我觉得这里字幕错了,应该是discussing(v.讨论;论述;谈论;商量;详述)
We're trying to use the minimum(adj.最底限度的) number of physical registers, or at least use the registers that we have.
And, we are trying to do as much reuse as possible, OK? 
So, if we have an instruction like this<here(1)>, then we would say that, the live range of register r1 is going to be 
this much. 
So, this is what we call the live range(1) of register r1.
So, register r1 got defined here(2), and it got used here(3) and here(4).
So, the live range of our register is the distance between the definition, this<here(2)> is where we're defining it.
We're putting a value in it(r1). OK, we're storing a value or a result in it(r1). So, it's defined here(2). And, it's 
used here(3) and here(4). So, this is the live range of register r1. So, register r1 has a long live range(1) with this 
instruction in red(2).
While.. what is the live range of register r2?
>>It's just those two below it.
Yes, just the three, the first three instructions.
So, this is the live range(2) of r2.
>> would the live range start at the second instruction?
We're using it here, right? (不重要,指错位置了,马上更正...不用管)
Oh.. yes. You're right. Yes. So, this is where we are starting, with the definition.
Yes. So, this is the live range of r2.
So, this is the definition(1), and this is the last use(2).
While for r1, this<here(2>) is the definition and this<here(1)> is the last use.
OK. Question on that concept of a live range? OK. So, the point here is when we have longer live ranges, we have a 
stronger competition for registers. So, we say that we have high register pressure.
In fact, by register pressure, we mean the number of registers that are live at a certain point.
We'll say more about this later. But, let's now get back to this example.
But, let's now get back to this example.

So, let's now get rid of(摆脱) this red(2) instruction. Now, we get rid of this instruction, then the live range of r1 
now becomes this(如下图). Live range of r1.
live  +-  load a -> r1
range |   load b -> r2                -+ live range of r2
of r1 +-  mult r1, r2 -> r3 here(1)   -+                     -+
          load c -> r4             -+                         |
    .(1)  load d -> r5              | live range of r4        | live range of r3
  this(1) div r4, r5 -> r6 .(2)    -+                         |
  this(2) add r3 here(2), r6 -> r7                           -+
          store r7 -> x
OK. So now, we can do, again, can use P1. 

Now, for r4, what can I use?
Let's say we will always use the physical register with the smallest number that is available.
So, what will be available for us to use for r4?
>> P2.
P2. Exactly. So, for r4, we'll use P2.
  r1 -> P1
  r2 -> P2
  r3 -> p1
  r4 -> P2
  r5 -> P3
  r6 -> P2
  r7 -> P1
Of course, if we use P3, it would be perfectly fine. but we're trying to use the minimum number of 
registers here.
So, r4 we can use P2. For r5.. We must use P3, right? (提示和备忘:看P1和P2保存的虚拟寄存器的生命周期)
Because here, at this point.. At this point(.(1)), we have register r3 and r4, both of them are live(adj.活的,有效的).
So, here when you get to(达到) this point(.(1)), r3 is live, because it got defined here(1), and it's used here(2). So, 
this is the live range of r3. Right? 
And, the live range of r4 is this(live range of r4). 
So, at this point(.(1)), r3 and r4 are still live, so we can not use the registers of, P1 and P2 for r3 and r4.
We must use a third register. Here. OK. So, for r5, we must use P3.
OK? And, for r6, now..so, which registers are no longer live at this point? r4 and r5, right?
r4 and r5, after we execute the divide, we no longer need them. 
(理解:执行到最后使用寄存器的抽象汇编指令时就不需要了,
纯属个人附加的理解->操作数送入ALU后就可以复用该物理寄存器了.刚好下面紧接的讲课内容验证了我的这个想法。。。)
By the way, we are assuming that, this(1) instruction, you can reuse the reigsters that you are using for the 
input for source operand of an instruction.
Because you will read these values(保存在r4和r5的值) before you write to the register that will hold the result. 
So, that's why we are able to reuse the registers that are used for the source operands of an an instruction.
(理解+润色翻译：重用一条指令中用于源操作数的寄存器[虚拟寄存器映射的物理寄存器])
So now, r4 and r5, they're not used below(备注：该点下面且包含该点) this point(.(2)), so we can just reuse 
either(det./pron.两者中任何一个) P2 or P3. We will reuse P2.
And now, for r6.. so, r3 is in P1. r6 is in P2. Right? And, in fact, after we are done with this(2).. at this point, 
after we're done with r3 and r6, we can use any register that we want, right?
(个人理解：根据示例中给出的抽象汇编指令这么来理解,执行到this(2),该指令为r7寄存器的定义,其他所有虚拟寄存器的最后使用
  ->[个人推测的规律(待验证)：寄存器定义,寄存器用于保存目的操作数; 寄存器使用,寄存器用于源操作数],
  ->[虚拟寄存器的定义(即第一次出现)意味着物理寄存器的分配,用于寄存器的写入;
     寄存器的使用,用于对寄存器的读取,最后使用+用于读取,所以读取后该虚拟寄存器映射的物理寄存器就可以再次用于分配了;]
因此r7可以使用任一我们想要使用的物理寄存器.)
So, for r7, you can use anything that you want.
And, let's stick to the rule of using the smallest number. So, we'll just use P1 for r7. 
And then, we store P1 into x. 
OK? So, in this case, we managed to map virtual registers into physical registers.
  P1, P2, P3
And, we could do it wit three physical registers. 

Now, let's assume that we only have two physical registers.
So now, assume two physical registers. P1 and P2.
  P1, P2
So, that's all that you have. Now, let's redo it with two physical registers, see what happen.
        load a -> r1
        load b -> r2      
        mult r1, r2 -> r3 this(1)
        load c -> r4     
.(1)    load d -> r5     
here(1) div r4, r5 -> r6 
here(2) add r3, r6 -> r7 
        store r7 -> x
        -- add r1, r7 -> r8 -- (that red instruction that I deleted(1))
OK. So now, we can do, r1 in P1. And, r2 in P2. There is no other way of doing it. We have two of them.
  r1 -> P1
  r2 -> P2
So, we put this(r1) in P1, and this(r2) in P2.

Now, we have this(1) multiply and we need a register for the result. Without that red instruction that I deleted(1),
we can use the reigster for r1 or r2, because we assume that is consumes(v.消耗) the source operands before it writes  
to the destination operand.
So, we can put r3 in P1.
  r1 -> P1
  r2 -> P2
  r3 -> P1
Now, we need a register for r4, so we can put it where.. where can we put r4? 
>> Into P2?
In P2. 
  r1 -> P1
  r2 -> P2
  r3 -> P1
  r4 -> P2
Now, we need a register for r5. This is where we hit a problem.
Now, we need a register for r5. And, we have, r3 and r4.. at this point(.(1)). we are looking for a register for r5.
Now, r3 and r4 are the ones that have the registers. Are they live or dead?
>> They're live.
>> Live.
They're live, because we're using r4 here(1), and we're using r3 here(2). So, live means that they will get used 
later. So, both r4 and r3 are live. So, we can't reuse P1 or P2.
But, we need a register for this(r5). So, what we can do here is that, OK, we can say that this(1) r3, we can 
store it in memory. So, the register allocator in this case would generate a store instruction.
        load a -> r1
        load b -> r2      
        mult r1, r2 -> r3
        store P1 -> mem temp (修改前为temp)
        load c -> r4   this(1)  
        load d -> r5   this(2)  
  .(1)  div r4, r5 -> r6 this(3)
        add r3, r6 -> r7 
        store r7 -> x
        -- add r1, r7 -> r8 -- (deleted)
Store r3, which is, P1, store P1 in a temp.
So, this temp stands for(代表) a temporary memory location. Or, let's call it, to be explicit(adj.明确的), 
let's call it.. or mem temp.
OK? Now, we can use this register P1 for r5. So now we can say r5 goes to P1.
  r1 -> P1
  r2 -> P2
  r3 -> P1
  r4 -> P2
  r5 -> P1
So now, this(1) is P2, this(2) is P1.
When we divide, we divide P1 by P2, or P2 by P1.
And, we'll put the result in r6.
OK? Now, for r6, which register can we use?
Now, after this point(.(1)), r4 and r5, are they going to be live or dead? 
>> They're dead.
Dead. So, we can reuse either P1 or P2.
OK? Because r4 and r5 will no longer be needed. So, we can say, OK, 
  r1 -> P1
  r2 -> P2
  r3 -> P1
  r4 -> P2
  r5 -> P1
  r6 -> P1
we'll put r6.. in P1. Because both of these guys(r4,r5 at .(1)) will not be needed after this(3) instruction(之后且包含).
Now, we need r3. But virtual register r3 is no longer in physical register. So, it's not in a physical register. we stored 
it in memory. 
So, what do we need to do? What't the logical thing to do?
>> ???
Load it, yes. Bring it back from memory.
So, we load that memory, that temporary memory location, 
        load a -> r1
        load b -> r2      
        mult r1, r2 -> r3    
        store P1 -> mem temp  >>spill    sotres(1)
        load c -> r4   
        load d -> r5   
        div r4, r5 -> r6
        load mem temp -> P2    here(1)    loads(1)
        add r3, r6 -> r7 
            P2  P1
        store r7 -> x
we store it into.. which register? What are the options that we have?
We only have one option.
>> P2.
P2. Because P1 has r6 in it, and we still need r6. So the only option that we have is P2.
OK? So now, we can, add.. so, basically adding, P2 and P1. And then, we can put the result.. once we are done 
with this, we can use any register we want, so we can put r7 in P1.
  r1 -> P1
  r2 -> P2
  r3 -> P1
  r4 -> P2
  r5 -> P1
  r6 -> P1
  r7 -> P1
OK? Now, what's the point here?
The point here is that with fewer registers, when we have two physical registers instead of three, the register
allocator could not map each virtual register to a physical register. 
It had to do some storing and loading.
So, in this case, r3 did not get mapped, entirely(adv.安全;完整地;全部地) to a physical register.
In fact, we put it in a physical register temporarily. Then, we stored it in memory. 
Then, we loaded it again when we needed it.
So, this kind of store, we call it spilling(v.溢出). With the terminology(n.术语) of compliers and register allocation, 
this is a spill. So, we basically spilled it to memory, because we don't have enough room for it on the register file.
There is no room for it on the register file, so we spilled it to memory. And, here(1) we loaded it again.
So, this mean that when we have high demand for registers, the register allocator will not be able to accommodate(v.容纳)
all the virtual registers.
It will not be able to find a physical register for every virtual register. So, it will be forced to do spilling. It will 
be forced to spill some of them to memory.
And, obviously, these stores(1) and loads(1) that we are adding, that we call spill code, obviously these are slowing 
the execution. These are going to(将) slow the execution. We are adding instructions. These instructions are going to be(将要)
using machine resources, they will use functional units, 
and they will use all the machine resources.
And, they will be using the memory system. And, the memory system has a limited bandwidth. 
So, you are executing more load than(我觉得应该是and...) stores, and that's slowing you execution.
So, the job of the register allocator is to do the mapping with minimum spilling.
The job of the register allocator is to minimize these spills. It try to accommodate all the virtual register, and map them
to physical registers.
Any question on these concepts?
>> I have a question now.
>> With the architecture of today's processors, wouldn't when it spills over to write to memory, 
>> wouldn't it go into the L1 cache?
Oh, even if it does.. well, we can.. generally speaking, 
we can never control if something is in L1 cache or not.
But, most likely(adj.最有可能), when you are spilling, these spills are going to go to the stack. Yes. And, they're going to 
the stack, and they are getting reused.. reused within a short period(n.一段时间) of time, so they're very likely to be in the 
L1 cache.
So, it's, most likely, you will be hitting the L1 cache, or things will hit in the L1 cache, and these will not be cache misses.
But, even if you hit in the L1 cache, this is still slower than storing them in a register
L1 cache is still slower than the register. Accessing L1 cache will take two of three cycles, While accessing a register is 
instantaneous(adj.瞬间的). You can access the register immediately.
So, definitely(adv.确实) L1 cache is much faster than main memory. So, L1 cache you can access it in two to three to four cycles,
while main memory.. to access main memory, you need hundreds of cycles. So, it's two orders of magnitude(n.巨大) (它是两个数量级), 
  ->附加：在数学和工程领域，“orders of magnitude”（数量级）是一个特定的术语，它用来描述数值之间的差距。
on modern processors, it's two orders of magnitude faster than main memory, but it is still slower than a register.
And, you are also using, other resources. You are using the memory system. And, your memory system has a limited bandwidth. You 
are executing more memory operations. And, memory operations are expensive(adj.昂贵的). So, your memory system has a limited bandwidth.
Overall(adj.总体的,全面的,综合的;adv.全部,总体上;), you have limited resources on the machine. And, these instructions(添加的spill相关指令) 
that you are adding are just going to use some resources. They will use issued slots. So, it's an instruction that will use issued slots,
  ->附加：issued slot 发射槽位; issued v.发出;(正式)发给; slot n.狭槽;(投放或插入东西的)窄缝,扁口;
  ->     目前只能粗略抽象地理解："相关资源"就是槽位... 例如需要使用的物理寄存器?然后相关指令使用分配给其的"资源",载入数据到"资源":槽位的动作,是不是像发射...
  ->     有空去找偏硬件方面的书籍再去深入了解.例如：《计算机组成与设计 硬件软件接口》?
and that will use an issued slot that could be used for another instruction.
OK? Yes. So, we would expect(v.预计) spill code to hit on the cache most of the time, but even if it is, it will still slow the execution.
  -> cache的相关疑问解疑：回答在AI_Temp.md文件中:
  ->  问题目录：
  ->    问题：只有L1缓存是处理器私有的吗？
  ->    问题：从寄存器将数据写回内存需要经过L1到L2再到L3这个过程吗？
  ->    问题：所以数据写回L2或L3并不是写回主内存的必要条件？
  ->    问题：那可以跨过L2和L3直接将数据写回主内存吗？ *(重点疑问,直接看这个的回答就好,前面的问题是我当时问得不够精准.)
Of course, if it misses on the cache, it's a huge(adj.巨大的) performance hit(vt&vi.打,打击;n.击中,命中).
Any questions on the concepts of register allocation and spilling?
OK. So, is it crystal(adj.清楚的,如水晶般的;n.水晶,晶体) clear?
Yes. OK. So, unfortunately, optimizations tend(v.倾向,往往会) to increase the demand for registers. And, one of these optimizations that 
increase the demand for registers is the common subexpression elimination that we described last time. The other one is the other 
optimization in the backend, which is instruction scheduling. So, instruction scheduling.. increase the demand for registers. We only 
described it, described instruction scheduling briefly(adv.简要地), but we will describe it in greater detail next time, what instruction
scheduling is. So, the idea in instruction scheduling is hiding the latencies of the loads.
        load a -> r1
        load b -> r2      
   +-->
   |    mult r1, r2 -> r3     This(1)
   |    store P1 -> mem temp
   +--  load c -> r4   
        load d -> r5   
        div r4, r5 -> r6
        load mem temp -> P2
        add r3, r6 -> r7 
        store r7 -> x
This(1) is a multiply that uses the result of the load, so why don't we just execute another instruction while we're waiting 
for the load to complete? So, this is a brief and possibly(adv.可能,尽量) vague(adj.模糊的) description of instruction scheduling,
but next time we will explain instruction scheduling in detail. 
OK? Alright. So, I will see you.. any questions before we end the lecture?
OK. So, I will see you.


### Lecture 3: Compiler Overview(3) Instruction Scheduling Concepts(n.概念,观念)
So, today's lecture is going to be about instruction scheduling.
So, last time, what was the topic of last lecture?
>> Instruction selection.
Register allocation.
>> Yeah.
Okay. So, what is register allocation? How would you describe it?
>> Mapping the virtual registers into physical registers.
So, it's mapping virtual registers into physical registers.
And what's a virtual register? What's a virtual register?
>> It's part of the abstract assembly that's generated by the ???.
Okay, yes, it's part of the abstract assembly. But what does a virtual register correspond to? What does it represent?
>> It makes the assumption that you have an infinite amount(n.数量) of registers.
Yes, exactly. When you are using virtual registers, you are making the assumption that you have and infinite number of 
reigsters on the machine. The compiler assumes that there is an infinite number of registers on the machine.
Until it gets to(开始,到达) register allocation. So, in register allocation, now things get real.
Now, we're going to map these virtual registers into physical registers. 
So, a virtual register could be one of two difference things. So, what does a virtual register correspond to in that first program?
  ->翻译：因此，一个虚拟寄存器可能是两个不同东西之一。
>> A variable.
Yeah. So, it corresponds to a variable. But a virtual register could be something else. It doesn't have to be a program variable.
It could be?
>> Temporary value.
A temporary or an intermediate(a.. i.. 中间产物). A temporary variable that holds some intermediate result, okay?
So, that was the register allocation stuff.
    register allocation
          virtual register
         /            |
        |             |
        V             V
      variable    temporary variable(hold intermediate result):这个分支为个人根据理解补全的(原课程里板书没写完直接擦掉了...) 
  -> 附加个人理解：variable，使用程序里的变量(变量->内存地址)需要先载入寄存器;
  ->             temporary variable, ALU计算寄存器里的值(载入的程序量)后输出的结果值.
  ->             在抽象汇编(中间代码?中间表示之一?)中,是先假设机器有无限数量的寄存器(虚拟寄存器)的,
  ->             到了寄存器分配阶段,抽象汇编中的虚拟寄存器可能是两者之一。
  ->             程序中的变量或临时的变量(程序中的变量和计算结果即临时的变量).
  ->             
  ->             简要概括：
  ->              虚拟寄存器
  ->                两种表示：1. 变量; 2. 临时变量;
  ->                live range：活跃范围 由 虚拟内存定义 和 最后使用决定;
  ->                 虚拟寄存器 -map-> 物理寄存器
  ->              物理寄存器：1. reuse; 2. register pressure -> spill code;

Now today, we will be discussing instruction scheduling. Instruction was briefly described in the previous lecture.
today we're going to study it in detail. So, let's look at our.. the same example,
x equal a times b plus c divided by d.
  x = a * b + c / d;
                               
           assign                  this(1)
          /       \
         x         + 
                 /   \
                *     /    
              / \   /  \  
              a  b  c   d 

Let's look at this example. The abstract syntax tree for this is going to be, adding.. multiply, sorry. Multiply a and b.
And dividing c by d. And then adding the results. And then assigning to x.
Okay, So, now let's try some assembly code from virtual assembly for this(1).
So, let's stick to(坚持,坚持不变,继续使用...) the code that we wrote last time.
  <these instructions(1)>
  load a -> r1
  load b -> r2
  mult r1, r2 -> r3
  load c -> r4
  load d -> r5
  div r4, r5 -> r6
  add r3, r6 -> r7
  store r7 -> x
So, we have load a into register r1. By the way, last time we pointed out that this load is a shorthand for a load instruction
that takes a register and an immediate. So, this a here, represents what in the assembly?
So, in an actual assembly, this a will be an offset. So, it will be just a number, an integer. An offset relative the activation
record pointer. 
So, loading a into r1, loading b into r2.
Then multiplying r1 and r2. And putting the result in r3.
Then loading c into register r1.
loading d into register.. sorry. Into register r4, r5. And then dividing what by what?
r4 by r5. And putting the result in r6. Then adding? R3. Putting the result in r7.
Then storing r7 into x. 

Now the point is that when we have.. when we have code like this, some of these instructions(1), like 
the load instructions, some of them have long latencies(空闲潜伏期/延迟). And so, it takes a certain number of cycles for the result 
to become available(可用的,可获得的). 
So, if we assume.. let's assume that the following latencies. 
Let's assume that 
for the load, we have a latency of 3 cycles.
For a multiply, 2 cycles.
For a divide, 5 cycles.
And for store, 2 cycles.
And for the add, 1 cycle.
  Latencies
      load: 3 cycles
      mult: 2 cycles
      div: 5 cycles
      store: 2 cycles
      add: 1 cycle.
Usually we mention the ones that are not.. that are greater the one.
The instructions that have the latency(延迟) greater than one. And everything else will by default, has a latency of one.
(默认情况下，其他所有东西都有一个延迟。)
So, but in this case, we only have the add. So, we just mention it explicitly(明确地). 
Okay, so, with these latencies, let's try to count the number of cycles that this sequence of code is going to take.
  start    end  
  ----------------------------------------
    1       3        load a -> r1               this(2)
  ----------------------------------------
    2       4        load b -> r2               this(1)
  ----------------------------------------
                     mult r1, r2 -> r3          this(3)
  ----------------------------------------
                     load c -> r4
  ----------------------------------------
                     load d -> r5
  ----------------------------------------
                     div r4, r5 -> r6
  ----------------------------------------
                     add r3, r6 -> r7
  ----------------------------------------
                     store r7 -> x
  ----------------------------------------
So, let's do the.. let's try to count the cycles. And so, a start and end.
So, assuming that we start in cycle 1. This(load a) load will take 3 cycles. So, it's going to start in cycle 1 and end in 
cycle 3, okay?
Now, modern processors, offer you the option of executing instructions in parallel(并行的).
So, in.. what's the architectural feature that will.. that allows you to start a new instruction in each cycle?
Even if some instructions have not completed?
>> Pipelining it?
Pipelining, exactly. So, with pipelining, you may start a new instruction, even if some instruction have not completed yet.
(*重点:注意下面的语句,这是课程里这个例子的假设前提,不然会有很多疑问,假设这是每个时钟周期可以开始一条新指令的单发射CPU[流水线],再往下看还有另一个前提(下1),
下一条新指令要在不[数据或控制]依赖其它指令的前提下才能实现下一个周期开始该条新指令.)
And in this case, let's assume that we can start a new instruction, one new instruction with each cycle. So, this is a 
single-issue machine.
(附加:单发射（指令）机器,在同一时间内只能对一条指令进行解码和执行,解码和执行为流水线的两个不同阶段,每个阶段同一时间只能够处理一条指令[一条指令当前处于某个阶段])
We're assuming a single-issue machine. Where in each cycle, we can start a new instruction.
  Latencies           (个人添加:这里的cycle为时钟周期)
      load: 3 cycles   
      mult: 2 cycles   
      div: 5 cycles
      store: 2 cycles
      add: 1 cycle.
  single-issue machine
But assuming that this instruction is what?
In every cycle, we can start a new instruction, assuming what?
>> It's the instruction is independent of the previous instruction.
  (前提(1)->理解：在单发射机器中,假设在每个周期,我们可以开始一条新指令,这个假设的前提是当前指令的执行不依赖于前一个指令[数据依赖或控制依赖])
Exactly, exactly. That's the point.
So, as long as(这里翻译为 只要) we have an independent instruction. An instruction that does not depend on the instruction that is currently 
executing in the pipelining, we can start a new instruction.
So, here, with this(1) load, is it dependent or independent?
>> independent.
It's independent. It doesn't depend on this(2).
So, we can start it.. this is.. sorry, start. So, we can start it in cycle 2, because it's independent. And it's going to finish in cycle 4.
  ===> 暂停：(需要补的知识,CPU的单发射和多发射,深入理解流水线...计算机体系结构的知识点,理解后再继续课程...20240124)
  ===> 大概厘清了，准备可以继续开始课程了...(20240126)
Now, this(3) multiply, is it depend or independent?
>> Dependent.
It's dependent. In fact, it depends on both(this(2)和this(1)). 

And since we are talking about dependents. Compilers construct what we call the dependence graph for this. So, if this is.. let's number the
instructions. If this is instruction 1, instruction 2,3,4,5,6,7,8. 
      start    end  
      ----------------------------------------
1.      1       3        load a -> r1              
      ----------------------------------------
2.      2       4        load b -> r2               
      ----------------------------------------
3.      5       6        mult r1, r2 -> r3         
      ----------------------------------------
4.      6                load c -> r4
      ----------------------------------------
5.                       load d -> r5
      ----------------------------------------
6.                       div r4, r5 -> r6
      ----------------------------------------
7.                       add r3, r6 -> r7
      ----------------------------------------
8.                       store r7 -> x
      ----------------------------------------
Then the compiler would construct a data dependence graph. Which is very much, in this.. for this simple example, it's an inverted(adj.反向,倒置的)
abstract syntax tree, basically. 
So it's going to be 1 and 2. 1 and 2 are independent. And 3, depends on both 1 and 2. So, in this case, 3 depends on 1 and 2, 
So, in this case, 3 depends on 1 and 2, because it uses the results of 1 and 2. So, this our dependence, data dependence graph.
  Data Dependence Graph
    (1)     (2)
     \      /
      \    /
       ↘ ↙ 
       (3)
Okay. Now, the multiply is dependent on both of them. So, it can't start any earlier than cycle 5. So, cycle 5 is the earliest cycle 
from the multiply to start. Because it.. this(load b -> r2) completes in cycle 4, the result of this(load b -> r2) will be available
in register 2 and cycle 5. This is our assumption.
Now, this is going to take cycles 5 and 6. Now, the load, is it dependent or independent? Does it depend on the result of the multiply?
No, it's not consuming. It's not using the result of the multiply. So, it's independent. So, we can just start it in cycle 6. We don't 
have to wait for the result of the multiply. Okay, so, this is going to start in cycle 6.
So, instruction number 4 is independent.
    (1)     (2)     (4)           附加：例如(1) -> 1.
     \      /
      \    /
       ↘ ↙ 
       (3)
And the same applies to instruction number 5. So, this is going to be 6 to 8, 6, 7, 8. And this is going to be what? So, should this to 
9 or 7?
>> Seven.
Why 7?
>> It doesn't depend on ...
Yeah, it doesn't depend on the previous instruction. So, it can start in 7.
You can start instruction, as long as it doesn't depend on previous instructions. So, this is going to be 7 to 9, okay?
      start    end  
      ----------------------------------------
1.      1       3        load a -> r1              
      ----------------------------------------
2.      2       4        load b -> r2               
      ----------------------------------------
3.      5       6        mult r1, r2 -> r3         
      ----------------------------------------
4.      6       8        load c -> r4
      ----------------------------------------
5.      7       9        load d -> r5
      ----------------------------------------
6.                       div r4, r5 -> r6
      ----------------------------------------
7.                       add r3, r6 -> r7
      ----------------------------------------
8.                       store r7 -> x
      ----------------------------------------
And now, the divide, is it dependent or independent?
>> Dependent.
It's dependent. So, we have 4 and 5, they are both independent. And they feed(v.向..提供;喂养) 6.
    (1)     (2)     (4)    (5)     <附加：4和5指的是4.和5.行抽象汇编指令>
     \      /        \      /      this example(1)
      \    /          \    / 
       ↘ ↙             ↘ ↙
       (3)             (6)
So, 6 is fed(feed的过去分词和过去式) by 4 and 5. It uses the results of 4 and 5. This is our data dependence graph.
Which, in this example(1), it's just an inverted abstract syntax tree.

But generally speaking, it's not the case. The dependence graph is not.. it's more complex that that. But for this example(1), it is.

So, the divide is dependent. So, what should be the starting cycle for the divide?
>> Ten.
Ten. Then the divide is?(附加:指向Latencies的板书)
  Latencies           (个人添加:这里的cycle为时钟周期)
      load: 3 cycles   
      mult: 2 cycles   
      div: 5 cycles
      store: 2 cycles
      add: 1 cycle.
Five. So, it's going to be 10 to 14. Okay, 10, 11, 12, 13, 14.
      start    end  
      ----------------------------------------  the whole calculation(1)
1.      1       3        load a -> r1              
      ----------------------------------------
2.      2       4        load b -> r2               
      ----------------------------------------
3.      5       6        mult r1, r2 -> r3       r3(this(2))   
      ----------------------------------------
4.      6       8        load c -> r4
      ----------------------------------------
5.      7       9        load d -> r5
      ----------------------------------------
6.      10      14       div r4, r5 -> r6        r6(this(1))
      ----------------------------------------
7.      15      15       add r3, r6 -> r7
      ----------------------------------------
8.      16      17       store r7 -> x
      ----------------------------------------
        17 cycles
Now, the add, is it dependent or independent?
>> It's dependent on the divide.
It's dependent on the divide. It's dependent on this(1) and this(2), actually.
This dependence is not a big deal because the result(3. this(2)) is already available in starting cycle 7.
But this(1) is the dependence that is going to cause a delay. So, we can't start this at any earlier than cycle 15. Because it's(7.) 
dependent.
And so, our dependence graph here, 7 is dependence on 3 and 6.
    (1)     (2)  (4)    (5)     附加：例如这里的(1) -> 抽象汇编指令的1.
     \      /     \      /      this data dependence graph(1)
      \    /       \    / 
       ↘ ↙          ↘ ↙
       (3)          (6)
        \            /
         \          /
          \        /
           \      /
            \    /
             ↘ ↙ 
             (7)
              |
              V
             (8)                 
And 8(8.) is dependent on 7(7.).
So, this data dependence graph(1) is very intuitive(adj.直观的,直觉的,易懂的.), right?
Now, the result(7. r7) is going to be available in 16. And 16 and 17, we will finish the store. 
So, we needed 17 cycles. Now, this is just, this is just an estimate(n.估计;v.估计,估算).
By the way, this is just an estimate that the compiler is making. The compiler is making this estimate, based on the latencies.
Based on instruction latencies. And based on a certain processor model.
So, it depends on.. this whole thing depends on the.. these calculations depend on the processor model.
  (附加：these calculations指的是计算这些抽象汇编指令,即1. 到 8.)
But, one of the assumptions that we are making here in our calculations, are we here assuming that the loads are going to hit or
miss in the cache?

(附加：开始添加新的假设前提...缓存命中和未命中时的延迟)
So, we are assuming that each load takes a couple of cycles or 3 cycles. This is assuming a cache hit.
Because the cache miss, bringing something from main memory, takes how many cycles?
>> Forever.
No, it's hundreds. So, in a modern processor, it's going to take hundreds of cycles. So, it's two orders of magnitude more than
  (orders n.顺序,次序,秩序...; orders of magnitude 数量级; 我个人觉得从orders为次序的角度来理解该术语...)
bringing something from the cache. 
So, by the way, everything that the compiler does, it's not.. it does not necessarily have to match the reality, when you run the 
program. So, the compiler makes certain calculations and certain estimates. But these estimates are not going to be 100% true. The 
compiler can make guesses(n.猜测) and estimates(n.估计), in terms of performance(在性能方面). But in terms of correctness(n.正确性),
the compiler cannot guess. So, the compiler must be correct. Must generate correct code. But in terms of performance, it can make
some guesses and estimates. 
So, in this case, for example, one of the implied(v.暗示) implicit(adj.不直接言明的) estimates is that, these loads are hitting in 
the cache.
In fact, if they miss, then, this whole calculation(1) will not make sense(没有意义).
  (附加：sense n.感觉,感知,感官(视,听,嗅,味,触) -> 延伸到更抽象的认知层面
  个人胡乱理解：物的sense，意义，合理，存在即有意义。。。存在即合理？？？！！！R￥#%￥#%@#。。。)
Because, we need hundreds of cycles for a cache missing memory operation.
>> Can the compiler know if it loaded in that memory location before, so it would be in the cache?
Okay, so, this is a good question.
If the compiler has loaded this before, then we can assume it's in the cache. Can we always assume that something that has been
loaded before is in the cache?
>> No ???
No. Because it may have gotten locked out of the cache by something else. And in fact, if you are at the certain point in a 
program. So, this is your program. And you are here(1) in the program. You have a load somewhere here(1) in the program.
              +------------------+
              |     .            |    --+    starting point(1), entry point(1). 
              |   / | \          |      |
              |   + + +          |      |
              |   | | |          |      |
              |   | | |if else   |      |
              |   | | |if else   |      |
              |   | | |          |      +-> multiple tasks(1)
              |   | | |          |      |
if else(1)    |   | | |          |      |
              |   | | |          |      |
              |   | | |          |      |
              +---+-+-+----------+      |
              |   V V V          |      |
              +------------------+    --+
              | load .           |            here(1)
              |                  |
              |                  |
              |                  |
              |                  |
              +------------------+
In general, you program would have lots of conditioners. Lots of if else. Which means that there are many, many paths that
can get you from the starting point(1), the entry point(1) to this point. So, there are multiple tasks(1).
Because on the way, there are lots of if else(1). Lots of conditioning, conditionals.
Now, not knowing the path, means you don't know what is in the cache.
Because what's in the cache, the state of the machine, is dependent on the path that took you from the entry point(1) to 
this(here(1)) load.
Okay, so, for example, you could have an if statement, if execute a huge(adj.巨大的) piece of code. So, we don't know if 
that huge piece of code got executed or not. Because in general, the compiler doesn't know if an if statement, you know,
evaluates(v.评估) to two or four(应该是true or false,应该是字幕错误). So, remember that you are doing all of this at compiled 
time. Keep in mind that you are not running this program. We are compiling. We're just translating. We're translating high
level machine code, high level source code into machine code. That's all we are doing. We are not running the program.
  (个人理解：中心思想->编译器对if..else..语句中哪些语句块在程序运行时哪部分会执行是不知道的[一个if..else..语句有两条路径],
  因此无法预测哪些变量是否在和还在cache中.)

So, you can model the processor. Like I said, you can always model the processor. But this model will never.. compile time
information will never be 100%. It will never match runtime information 100%. Okay, yes?
>> Even if you did have the instruction immediately prior(adj.先前的) pulling something in from memory, you can't really 
guarantee(v.保证,担保;n.保证,担保;) it's in the cache. Because you could have pass switching going on. So, if another process 
get loaded, that could kick(v.踹,踢;n.踢;) the stuff out of the cache.
  (AI翻译和解释：在编译原理的情境下，这句话可以翻译为：“即使你在执行从内存加载数据的指令之前立即进行了操作，也无法真正保证数据已经被加载到缓存中。因为
  可能会存在上下文切换（进程切换）的情况。所以，如果有另一个进程被加载运行，它就可能将已有的数据从缓存中驱逐出去。”这段话描述了在多进程环境下，编译器无
  法绝对确保某个数据一定存在于缓存中的问题。由于操作系统进行上下文切换时，不同进程对缓存的使用可能导致缓存内容的变化，先前加载的数据可能因缓存替换策略而
  被移出缓存。)
Yes. There are so many factors that you don't know. So, the compiler can only guess. And this guess may or may not be right.
But remember, that the compiler is allowed to guess, when it comes to performance. It's not allowed to guess, when it comes 
to correctness. The code has to be correct. And the performance, we want it to be as good as possible.
Okay, so now this is the.. this is our estimate. Based on this simple machine model, our estimate of the number cycles.
Now, a compiler would do instruction scheduling. By the way, we haven't yet, said what an instruction schedular would do. An instruction
schedular would look at this(1).
图表(1)
  this(1)
      start    end  
      ----------------------------------------  
1.      1       3        load a -> r1              
      ----------------------------------------
2.      2       4        load b -> r2               
      ----------------------------------------
3.      5       6        mult r1, r2 -> r3        
      ----------------------------------------
4.      6       8        load c -> r4
      ----------------------------------------
5.      7       9        load d -> r5
      ----------------------------------------
6.      10      14       div r4, r5 -> r6        
      ----------------------------------------
7.      15      15       add r3, r6 -> r7
      ----------------------------------------
8.      16      17       store r7 -> x
      ----------------------------------------
        17 cycles
      -------------
And it would try to reorder these instructions to minimize the number of cycles. So, an instruction schedular, in this case,
may notice that these loads have long latencies. And these loads are independent. So, I can, I can execute.. I can execute all
the loads in the very beginning. And then do everything else afterwards(adv.之后). So, an instruction schedular may do this.
  load a -> r1
  load b -> r2
  load c -> r4
  load d -> r5
  mult r1, r2
Load a into r1. Load b into r2. Load c into r4. So, we are just, you know, looking at these instructions and reordering them.
So, we are reordering these instructions, in an attempt to minimize the number of cycles needed to execute this sequence of 
code.
SO, Loading c into r4. we put instruction 4(4. ..), before instruction 3(3. ..). That's what we did.
Loading d into r5. Then we multiply r1.

>> Wouldn't it be faster to do run the division first? Division takes longer.
Okay. Well, that's a good point, yes. So, the division takes longer than the multiply. But the question is, will the results 
of the division be available or not? So, well, in fact, what you are suggestiong is that the division is slower. So, we can 
do.. we load.. we can load c and d, before loading a and b. Because c and d are the operands that the division needs. 
And we can do this. And do that before the division, then do the division, yes. So, we can do that.
So, we are not, by the way, not trying to construct the optimal schedule. Because as we will see, there is no known algorithm
that can always find the optimal schedule. But what you are suggestiong is good, yes.

Now, let's stick to this, just, you know, putting the loads in the beginning. And then doing the multiply. Then doing the divide,
which is r4 an r5. So, we put this load into r5. r1 and r2, we put this into r3(补全上面mult r1, r2 -> r3的板书). And r4 and r5, We
put this into r6. And we add what? r3 and r6. And put this in r7. Then store r7 into x.
  load a -> r1
  load b -> r2
  load c -> r4
  load d -> r5
  mult r1, r2 -> r3
  div r4, r5 -> r6
  add r3, r6 -> r7
  store r7 -> x

Okay. So, now let's count the cycles. So, this is start and this end.
图表(2) More ILP                      个人添加备注：图表(2)原件
    start     end
  ------------------------------------------------------
     1         3          load a -> r1                        this(0)
  ------------------------------------------------------
     2         4          load b -> r2                        this(1)
  ------------------------------------------------------
     3         5          load c -> r4                        this(2)
  ------------------------------------------------------
     4         6          load d -> r5                        This(3)
  ------------------------------------------------------
     5         6          mult r1, r2 -> r3                   this(1)+3
  ------------------------------------------------------
     7         11         div r4, r5 -> r6                    it(1)
  ------------------------------------------------------
     12        12         add r3, r6 -> r7
  ------------------------------------------------------
     13        14         store r7 -> x                       this(4)
  ------------------------------------------------------
  14 cycles
  ---------
  Latencies           
      load: 3 cycles   
      mult: 2 cycles   
      div: 5 cycles
      store: 2 cycles
      add: 1 cycle.   
  single-issue machine
  instruction-level parallelism (ILP)
So, this is going to start in 1 and end in 3. And this(1), is it dependent or independent?
Independent. So, it's going to start in 2 and end in 4. And this(2)? Also, independent. So, it's going to start in 3 and end in 5.
This(3) is also independent. So, it's going to start in 4 and end in 6.
Now, the multiply is dependent. What is it dependent on? 
>> r1 and r2.
It's dependent on r1 and r2. So, r2, the result in r2 would be available in cycle 5. So, we can do cycle 5 here, okay? Because r2 
is this[position: this(1)的r2 和 this(1)+3的r2]. So, r2 would be available in cycle 5. So, the multiply can go ahead in cycle 5, 5
and 6.
Then the divide is dependent on r4 and r5. So, the lastest result that it needs is in r5. And this(r5) will be available in cycle?
Seven, right? Yeah. So, the divide is going to start in cycle 7. Because it(1) needs the r5.
So, 7, r5. So, 7 and it's going to be 6 plus 5 is 11. So, it's going to end in 11.
Now, the add is dependent on the divide. So, it will have to wait until cycle 12. And 12 to 12. And this(4) is at 13 to 14.
So, in this case, 14 cycles.
So, in theory here, the compiler has reduced the number of cycles need to execute this from 17 to 14(图表(1)->图表(2)).
And, it did this by basically hiding the latencies of the loads. Or hiding the latencies of these independent instructions.
  (附加：隐藏独立指令的延迟,在课堂的上下文理解为通过指令调度来将这些(流水线)延迟(数据或控制依赖导致的流水线停顿)隐藏.
   更精准地说,隐藏(指令)流水线延迟. 简单来说,就是课堂这部分所讲的就是通过指令调度来减少流水线的停顿.
   区分两个概念 指令延迟(单条指令本身执行[这里的执行指的是取指->写回全过程]的延迟) 和 流水线延迟(流水线停顿[流水线延迟,流水线阻塞]).
   减少依赖指令"等待"的时间.
   AI_Compiler.md,有指令延迟和指令流水线延迟这两个概念吗？
   AI_Compiler.md,编译原理中,隐藏指令的延迟是什么意思？)
By placing some independent instructions. It placed independent instructions between this(this(1)) load and the consumer of the 
load, which is the multiply. So, the multiply consumes the result of the load(this(1)). The load(this(1)) has a long latency. So,
in this long latency, we put some independent instructions(指的是this(2)和this(3)). This is the point.

By doing this, we are doing more instruction level parallelism(n.平行;计算机术语一般翻译为并行). So, we are doing instruction level 
parallelism. We are doing more instructions in parallel.
Why? Because here, basically we are doing all of these loads(图表(2) 4条load指令), all of these 4 loads there, executing in 
parallel(adj.并行的;adv.并列地;计算机术语统一并行的/地), okay? They are in the machine and they are executing in parallel.
  (附加：理解这里需要了解流水线深度这个概念.)
Here(this(0)和this(1)), we are only doing 2 loads in parallel, okay?
So, in this case, this is more, instruction level parallelism. So, this(图表(2)) has more ILP. More instructions executing in parallel.
图表(2) More ILP                      个人添加备注：为了方便查看,复制了一份图表(2)...
    start     end
  ------------------------------------------------------
     1         3          load a -> r1                        this(0)
  ------------------------------------------------------
     2         4          load b -> r2                        this(1)
  ------------------------------------------------------
     3         5          load c -> r4                        this(2)
  ------------------------------------------------------
     4         6          load d -> r5                        This(3)
  ------------------------------------------------------
     5         6          mult r1, r2 -> r3                   this(1)+3
  ------------------------------------------------------
     7         11         div r4, r5 -> r6                    it(1)
  ------------------------------------------------------
     12        12         add r3, r6 -> r7
  ------------------------------------------------------
     13        14         store r7 -> x                       this(4)
  ------------------------------------------------------
  14 cycles
  ---------
  Latencies           
      load: 3 cycles   
      mult: 2 cycles   
      div: 5 cycles
      store: 2 cycles
      add: 1 cycle.   
  single-issue machine
  instruction-level parallelism (ILP)

Now, there is a cost for everything. So, the cost here(图表(1)) is using more registers. This schedule here(图表(1)), needs more
registers than this(图表(2)) schedule.

So, let's analyze the registers(根据上下文这里指的是虚拟寄存器) that are live, for each one of these two schedules.
So, here let's look at... okay. Okay. So, this is live registers. So, now live registers here. After executing this(图表(1-1):this(0)),
this instruction(图表(1-1):this(0)), r1 will be live. So, it's defined here(图表(1-1):this(0)) and it's used here(图表(1-1):this(1)).
  (附加：所谓define定义就是载入/写入到寄存器,所谓use使用就是读取寄存器.指令中使用即读取该寄存器的数据后,就可以重用该物理寄存器了,哪怕在同一指令内使用.
        肯定要先写入数据才能读取啊。。。即所谓定义和使用...
   AI解惑,AI_Compiler.md：
      问题：编译原理中，虚拟寄存器的define和use是指什么？)
After executing this(2), live register are going to be r1 and r2. Both r1 and r2, at this point(this(1)), are live. Because they have 
been defined and they will be used later. And later is here(this(1)). This(this(1)) is the last(use). Now at this(. <- this(1)) point, 
after the executing of instruction 3(3.), which registers will be live? 
>> r3. r3 only.
What happened to r1 and r2? They are no longer live because they are not used below instruction 3. So, instruction 3 is the last use of 
r1 and r2. So, if we have something that uses r1 here(here(1)).. if we have something that uses r1, then r1 would have continued to be 
live. But since there is no use for it below this point(. <- this(1)), this(this(1)) is the end of the live range. So, r1 and r2 are not
live. only r3 is live here(this(1)).
And with this(this(3)) instruction, what will be live? And with this(this(4)), r3, r4 and r5.
And with the divide(this(5)), which instructions will be live?
>> r3 and r6.
Yes, r3 and r6. Because you still need r3 in the next instruction. Yeah, you still need r3. You still need it here(this(6)). So, r3 is 
live. But r4 and r5 will not be live after this(this(5)) instruction. They are not used below, okay? So, only r3 and r6 are live. And 
here(this(6)), r3 and r6 are used. So, they are no longer live, only r7 is live. And here(this(7)), nothing is live.
图表(1-1) -> 图表(1)的扩展live reigsters版本...    
schedule 1
   start    end                                live regs
   ---------------------------------------------------------------------  
1.     1       3        load a -> r1           r1                             this(0)
   ---------------------------------------------------------------------  
2.     2       4        load b -> r2           r1, r2                         this(2)
   ---------------------------------------------------------------------  
3.     5       6        mult r1, r2 -> r3      r3     (.)                     this(1)
   ---------------------------------------------------------------------  
4.     6       8        load c -> r4           r3, r4                         this(3)
   ---------------------------------------------------------------------  
5.     7       9        load d -> r5           r3, r4, r5                     this(4) peak register pressure
   ---------------------------------------------------------------------  
6.     10      14       div r4, r5 -> r6       r3, r6                         this(5)
   ---------------------------------------------------------------------  
7.     15      15       add r3, r6 -> r7       r7                             this(6)
   ---------------------------------------------------------------------  
8.     16      17       store r7 -> x          none                           this(7)
   ---------------------------------------------------------------------  
                        use r1                                                here(1)
     17 cycles    needs 3 physical registers
   -------------

Okay, Now, if we look at the register usage(n.使用) for the other schedule. Live registers. So, for this(this(0)), the live registers 
are r1. Now(this(1)), r1 and r2.
>> r1, r2, r4. (this 2)
Yeah. r1, r2, r4.
And here? r1, r2, r4 and r5.
Here(this(4)), we are using r1 and r2. And this(this(4)) is the last use of them. So, r1 and r2 are no longer live.
So, we are left with r4, r5 and r3, has also become live.
Here(this(5)), we're using r4 adn r5. So, r4 and r5 are no longer live.
But now we have r3 and r6. And here(this(7)) we are using r3 and r6. And only r7 is live. And here(this(8)) we have none.
图表(2-1) schedule 2
                             live regs                                        
  ------------------------------------------------
1.      load a -> r1        r1                       this(0)
  ------------------------------------------------
2.      load b -> r2        r1, r2                   this(1)
  ------------------------------------------------
3.      load c -> r4        r1, r2, r4               this(2)
  ------------------------------------------------
4.      load d -> r5        r1, r2, r4, r5           this(3) peak register pressure
  ------------------------------------------------
5.      mult r1, r2 -> r3   r4, r5, r3               this(4)
  ------------------------------------------------
6.      div r4, r5 -> r6    r3, r6                   this(5)
  ------------------------------------------------
7.      add r3, r6 -> r7    r7                       this(7)
  ------------------------------------------------
8.      store r7 -> x       none                     this(8)
  ------------------------------------------------
    14 cycles    needs 4 physical registers
  --------------  
Now, the question is, what's the minimum number of physical registers that you need here? And what's the minimum number
of physical registers that you need? So, this is schedule 1(图表(1-1)). And this is schedule 2(图表(2-1)).
So, here, this is your peak(schedule 1 this(4))(n.山峰,最高点). This is the maximum. So, at this point(schedule 1 this(4)),
you have 3 registers live, that are live. So, this(schedule 1 this(4)) is you maximum or you peak register pressure. At 
this(schedule 1 this(4)) point, you need at least 3 registers. Because at this point(schedule 1 this(4)), you have 3 values
that are live. So, you need 3 physical registers here(schedule 1 this(4)), at least. So, this(schedule 1 this(4)) is your 
peak register pressure.
While here(schedule 2), what's our peak?
>> Four.
Four. So, our peak is here(schedule 2 4. <-> this(3)). So, this is the peak register pressure. So, the difference between this
schedule(schedule 1), this schedule(schedule 2) is.. schedule 1 is 17 cycles. But needs 3 physical registers. Seventeen cycles
and it needs 3 physical registers. Schedule number 2 is 14 cycles. And needs 4 registers.
So, basically by reducing the number of cycles, requires more registers. Nothing comes for free. And this is very intuitive.
Because here(schedule 2) you are doing more things than(字幕应该错了,应该为in) parallel. You are doing 4 instructions in parallel. 
You need 4 reigsters.
  (附加： 
    AI解惑,AI_Compiler.md:
    问题：并发和并行是什么,有什么区别？
    回答概览：
      并发指的是在同一时间段内能够处理多个任务的能力;
      并行则是指多个任务在同一时刻真正地同时执行;
    问题：4 instructions in parallel如何理解？
    回答概览:
      1.支持多发射技术的处理器,可以在一个时钟周期内启动并处理多条独立的指令;
      2.在单个CPU核心内部,通过指令级并行Instruction-level parallelism, ILP技术;
    问题：详细解释一下指令级并行ILP技术。(instruction-level parallelism这个知识点必须要了解,是基础知识.)
    回答概览:
      流水线技术,超标量技术,乱序执行,分支预测,推测执行,软件编译优化.
  )
  (个人理解：各指令的延迟可看成是恒定的...待补全)
You need containers for these to stir(v.搅拌,(使)活动;n.搅拌;) the result of these. So, it's just like you're cooking, for example.
If you are trying to cook and you trying to cook in parallel. And if you want to cook, meat, rice and vegetables at the same time.
Then you need at least 3 containers, right? You cannot do it with 2 containers. If you have 2 containers, you cannot cook 3 things
at the same time. And what is happening here(schedule 2 1. - 4.), these 4 instructions are cooking. So, this is instruction 
11(字幕错误,应该为level) parallelism. So, they are cooking at the same time. So, we need containers. For more parallelism, you need 
more containers. So, that's why this is intuitive.
  (AI解惑：AI_Compiler.md.
    问题：编译原理中，For more parallelism, you need more containers. So, that's why this is intuitive 这句话该如何理解
    回答概览：
    这句话的意思是，在进行并行处理时，若要支持更多指令同时执行而不受限制，就需要有足够的硬件资源（如寄存器）作为数据容器，以便在执行过程中容纳不相关指令的
    操作数和结果。由于每个独立运行的指令都需要有自己的空间来保存数据，因此拥有更多的容器对于提高并行性是很直观的——就像如果想要同时做更多的事情，就需要更多
    的工具或者工作台一样。
  )
Now, what does the compiler do?
So, should the compiler try to minimize the number of cycles and use more registers? Or should it reduce.. minimize the number of 
registers and.. so, minimize the number of registers and use more cycles? Or minimize the number of cycles and use more registers?
>> Doesn't that depend on the architecture of the processor?
Yes. Yeah, exactly. So, it depends on the architecture, on the target processor. It depends on how many physical registers you have
on the target processor that you are trying to generate code for.
So, in this case, for example, if you have four physical registers available, then you can go ahead and do this more in section level
parallels. And minimize the number of cycles. And that will work out well.
  (附加：section level parallels, 这里的section指的是程序中的某一段可独立处理的代码区域.
        这里的“section level”可能指的是函数、循环体或其他能够独立分析和优化的代码段。当有足够的物理寄存器用于暂存变量时，编译器能够更好地挖掘并实现这些段
        内的并行性，提高程序的运行效率。
   AI解惑：AI_Compiler.md:
   问题：问题：可独立处理的代码区域是什么意思？
   问题：编译原理中，So, in this case, for example, if you have four physical registers available, then you can go 
   ahead and do this more in section level parallels.这句话是什么意思？
   问题：编译原理中，section level parallels是什么意思？
   问题：有section level parallelism这个概念吗?
  )
  (暂停：补一些计算机体系结构的知识...例如 ILP等... 20240206
    单发射和多发射描述的是处理器在单位时间内能够处理多少条新指令的能力;
    AI_Compiler.md问题:
      1.单发射和多发射CPU与单核和多核CPU有必然关系吗？
      2.单发射和多发射与流水线的关系。
      3.多发射CPU是否存在多条流水线？
        回答概览：
        a.单条高度复杂的流水线;
        b.多条并行的流水线;
  )
But in general, this is a hard problem to solve. In fact, instruction scheduling.. any realistic(adj.现实的,实际的) formulation(n.(计划的)制定) 
of instruction scheduling is NP-complete [brief laughter?]. Okay? So, in fact, only minimizing one of this two objectives, with 
the number of cycles or the number of registers, is in general, NP-complete.
So, even if our only objective(adj.客观的,就事论事的,基于事实的;n.目标) is finding the ordering that will minimize the number of cycles.
Even then, with long latency instructions, that's an NP-complete problem. And even if our sole(adj.唯一的) objective is minimizing 
the number of registers, that's an NP-complete problem. 
So, what's an NP-complete problem? How would you define NP-complete? Yes?
>> It's equivalent to.. it's a class of problems that are all equivalent to each other. That have no polynomial time solution.
Yes. Yeah. So, there is no polynomial time solution. Which means there is no polynomial time algorithm that can always computethe optimal
solution to this problem. There is no polynomial time algorithm. Not having a polynomial time algorithm, means that the algorithms that you
have are exponential(adj.指数的;幂的;n.指数). Super polynomials. So, they're exponential or factorial(n.阶乘;adj.阶乘的;因数的). And exponential
and factorial algorithm take a long time.
  n = 100
  2^100
And for practical purposes, if you have, for example, a problem with, n equals 100. So, in our case, so, n is the number of instructions.
So, 2 to the power 100. So, this is a huge number. So, if you do the math for this, this you will get, millions of.. millions of billions
of years. So, it means that if you want to run an algorithm, an exponential algorithm that will be trying to find the best order, in this
case. It's going to take billions of years, in the worst case, okay? 
So, compiler, that's why compiler don't solve this problem exactly(adv.完全地,确切地). So, they use certain(pron代词.某些;) heuristics(n.启发式).
And these heuristics hopefully, work well, in most cases. But not in all cases. So, a good heuristic is a heuristic, an algorithm that
is not guaranteed(v.保证;adj.必然的) to(g..to短语.保证) give you the optimal solution.
But it gives you something close to the optimal solution, in most cases. This means that, there may be some, extreme(adj.极端的;n.极端) cases 
where this algorithm will not give you the best results.
重点标记*:
Now, if you are.. the question is, if you are designing an algorithm for instruction scheduling? Should you focus on minimizing the number
of cycles? Or focus on minimizing the number of registers? Now, again, this depends on the target architecture.
And in fact, if you are generating this code for an out of order processor, out of order processor. 
  out of order processor
What does an out of order processor do, in this case? Why is there, knowing whether the processor is out of order or in order, why does it
matter?
>> Because you have.. a processor will do what the schedule two will do automatically.
Yeah, exactly(adv.完全正确). So, hopefully, the out of order processor, the out of order processor is going to do this at runtime. Well, but 
it's not going to do a perfect job. It's going to do a good job. Nothing is perfect, by the way. The processor is not perfect. And the 
compiler is not perfect. So, nothing in the system is perfect. 
But an out of order processor will be trying to do something like this(schedule 2). You know, hiding the latencies. And in fact, it will 
be using internal(adj.内部的) registers that are not visible to the programmers, to achieve more paralleism.
So, if you have an out of order processor, then you better focus on minimizing the number of registers. You better focus on minimizing the
number of registers. Because as(由于) we have seen in the register allocation lecture(n.讲课;v.讲课;), last lecture. If you need more registers
than what the machine has, the register allocator is going to generate what? 
>> Stores and reloads.
Stores and reloads. And we call those what?
>> Spins. (应该为字幕错误,应该为spills)
Spin(应该为spill) codes. It's going to generate spin code. So, if your processor has out of order execution. And it has a good out of order 
execution engine, you better focus on minimizing the number of registers. Because minimizing the number of registers needed, is going to 
cause the register allocator to generate fewer spills. Fewer loads and stores. So, that's much more important than minimizing the number 
of cycles. Because the processor is going to try to minimize the number of cycles.
Now, will the processor do a perfect job at this? The processor has limitations. So, in terms of scheduling to minimize the number of 
cycles. So, can you think of a limitation of the processor?
>> The main one is once it gets done with the loads, it's dependent on the result. So, the multiply and divide, before it can go to the add. 
Well, in this case, yeah. So, it's.. well, in fact, we'll assume that the, the processor will have.. so, if you present(vt.提交) 
this(schedule 1 1.) to a processor, it's going to do the load. Then it will do the load. Then this multiply, is dependent on the load.
But because the multiply is dependent on the load, the processor is not going to stop. So, it's going to go.. and that's what out of order
exectution. So, it's going to start the load. While this multiply is waitiong for its results. So, at runtime, the processor will be 
starting the load, before the multiply completes. So, it's going to start the multiply. It's going to issue the multiply. But the multiply 
will not.. before the multiply completes, it's going to start the load, trying to do more parallelism. But what are the limitations of 
the processor? Why is the processor limited? I mean, why can't the processor do a perfect job? Can you think about limitation? Yes? 
>> It can only look so far ahead.
Exactly. It can only look so far. So, this is going to be in the reorder buffer. So, there is a buffer.
  +-----------------+
  | reorder buffer  | 64 96        (重排缓冲器)
  +-----------------+
These can look only at a limited scope, what can fit in a buffer, in a certain buffer. So, it's not going to look, at the entire program.
And that reorder buffer will have tens of instructions in it. Only tens of instructions. Maybe 64, 96. I don't know, maybe they have 
gotten bigger now. Maybe 128 or something. But that's about, the range for a reorder buffer. So, the processor can only look at this 
limited scope. Which is the size of the reorder buffer. 
While the compiler, can look at the bigger piece of code. Even though there are limitations on the compiler side, too. So, the compiler..
there is no compiler that schedules a whole program at once. Because Scheduling a whole program at once is just too complicated(adj.复杂的).
  (意译：因为一次性调度整个程序实在太复杂了。)
But you don't expect the compiler to schedule, a million-line program. And, you know, move the, you know, the line number one, put it at
the end. So, it's not going to do this at the scope of the whole program. It's going to divide the program into pieces, into scheduling
regions(n.区域). And these scheduling regions are limited in size, too. But they can be bigger than the reorder buffer on the hardware.
So, the size of the scheduling region, the compiler may look at hundreds of instructions. And sometimes thousands of instructions, at 
the same time. So, it can do thousands, sometimes.
But not.. but what about what?
>> Sorry to interrupt you. But I was thinking, what about the.. a lot of processors today have jumped prediction(指的是分支预测).
>> So, how does that factor into this, too? Although, we don't have any branches in this code?
Yeah, So, branches make things even more complicated. They make them more complicated for the hardware. And they make them more complicated
for the compiler. Because.. in fact, having branches is one of the resons why a compiler cannot look at the big piece of code.
Usually, the compiler stops at the branch. So, it's branches define the boundaries(n.边界,界线) of scheduling regions.
You know, usually a compiler does a basic block. But it's something that we will be introducing later.
  basic block
And the same thing for the machine. If the hardware starts to execute beyond the branch. So, that's speculative(adj.推测的) excution(推测执行).
That may get.. it can not commit it, right? The hardware cannot commit what gets executed, after conditional branch. Until that branch
is the result.
So, again, the branch is an obstacle(n.障碍) for the hardware, as well. The branch complicated things. But all meaningful programs have
branches, have conditional branches. Can you imagine a meaningful program without if else? So, it's not going to be very useful.
You have the if else and you have the loops. The loops are also backward branches(循环也是向后分支...). So, without conditioners and without
loops, you're not going to write a meaningful program, a useful program. So, a useful program makes things complicated.
All right, so, now we have, we know what the instruction scheduling is. We know what the register allocation is. And they conflict with
each other. So, minimizing the number of cycles, conflicts with minimizing the number of registers. And this is typical for compiler 
optimizations to conflict with each other, to interact with each other.
So, improving something, can make something else worse. That's why optimizing code, generating, you know, good code, efficient code,
is a non-trivial(不平凡的) task. There are lots of tradeoffs(n.交易). Lots of conflicting objectives(目标). And that's why it's a very
complicated process. And, there is no compiler that can generate perfect code all the time. Compiler hopefully generate good code, in
most cases.
Okay, Any questions? So, this.. yeah?
>> Are out of order processors coming?
Yeah. So, Intel x86, is an out of order processor. So, that's the prominent(adj.突出的) processor on desktop and laptops and work stations.
So, Intel x86 is out of order.
Even Arm processor, the modern Arm processor are out of order, these days.
>> inaudible???.
Yeah. Yeah and of course, Intel and AMD, they are both out of order.
Okay, other questions? Okay, so, basically this ends our extended overview of compilers. So, what we have covered so far is the what?
You know, what compilers do. And what the different parts of the compiler do. We haven't studied the low. You know, the algorithms that
are used. So, we started the concept of instruction scheduling. But we haven't started a specific algorithm for doing instruction scheduling. 
So, from next lecture, we will start studying the algorithms for the different parts of the compiler. And what will be the first part that 
we will be studying?
>> Scanning.
Yeah. What's the first part in the front-end? What's the first phase in the front-end? Scanning, Yeah. So, next time we will start studying
scanning. How scanning is done, using finite optometer(字幕应该错了,应该为automata) and regular expression, okay?












